{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train FCN8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;sys.path.insert(0, '..')\n",
    "import os\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from src.UNET import UNET\n",
    "import src.core as core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device cuda\n",
      "UNET(\n",
      "  (conv_layer_1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_1_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (conv_layer_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_2_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (conv_layer_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_3_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (conv_layer_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_4_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (conv_layer_5): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_6_transposed): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cat_6): Concatlayer()\n",
      "  (conv_layer_6): Sequential(\n",
      "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_7_transposed): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cat_7): Concatlayer()\n",
      "  (conv_layer_7): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_8_transposed): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cat_8): Concatlayer()\n",
      "  (conv_layer_8): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_9_transposed): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cat_9): Concatlayer()\n",
      "  (conv_layer_9): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (output): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), padding=valid)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 96, 96]           1,792\n",
      "              ReLU-2           [-1, 64, 96, 96]               0\n",
      "            Conv2d-3           [-1, 64, 96, 96]          36,928\n",
      "              ReLU-4           [-1, 64, 96, 96]               0\n",
      "         MaxPool2d-5           [-1, 64, 48, 48]               0\n",
      "            Conv2d-6          [-1, 128, 48, 48]          73,856\n",
      "              ReLU-7          [-1, 128, 48, 48]               0\n",
      "            Conv2d-8          [-1, 128, 48, 48]         147,584\n",
      "              ReLU-9          [-1, 128, 48, 48]               0\n",
      "        MaxPool2d-10          [-1, 128, 24, 24]               0\n",
      "           Conv2d-11          [-1, 256, 24, 24]         295,168\n",
      "             ReLU-12          [-1, 256, 24, 24]               0\n",
      "           Conv2d-13          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-14          [-1, 256, 24, 24]               0\n",
      "        MaxPool2d-15          [-1, 256, 12, 12]               0\n",
      "           Conv2d-16          [-1, 512, 12, 12]       1,180,160\n",
      "             ReLU-17          [-1, 512, 12, 12]               0\n",
      "           Conv2d-18          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-19          [-1, 512, 12, 12]               0\n",
      "        MaxPool2d-20            [-1, 512, 6, 6]               0\n",
      "           Conv2d-21           [-1, 1024, 6, 6]       4,719,616\n",
      "             ReLU-22           [-1, 1024, 6, 6]               0\n",
      "           Conv2d-23           [-1, 1024, 6, 6]       9,438,208\n",
      "             ReLU-24           [-1, 1024, 6, 6]               0\n",
      "  ConvTranspose2d-25          [-1, 512, 12, 12]       2,097,664\n",
      "      Concatlayer-26         [-1, 1024, 12, 12]               0\n",
      "           Conv2d-27          [-1, 512, 12, 12]       4,719,104\n",
      "             ReLU-28          [-1, 512, 12, 12]               0\n",
      "           Conv2d-29          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-30          [-1, 512, 12, 12]               0\n",
      "  ConvTranspose2d-31          [-1, 256, 24, 24]         524,544\n",
      "      Concatlayer-32          [-1, 512, 24, 24]               0\n",
      "           Conv2d-33          [-1, 256, 24, 24]       1,179,904\n",
      "             ReLU-34          [-1, 256, 24, 24]               0\n",
      "           Conv2d-35          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-36          [-1, 256, 24, 24]               0\n",
      "  ConvTranspose2d-37          [-1, 128, 48, 48]         131,200\n",
      "      Concatlayer-38          [-1, 256, 48, 48]               0\n",
      "           Conv2d-39          [-1, 128, 48, 48]         295,040\n",
      "             ReLU-40          [-1, 128, 48, 48]               0\n",
      "           Conv2d-41          [-1, 128, 48, 48]         147,584\n",
      "             ReLU-42          [-1, 128, 48, 48]               0\n",
      "  ConvTranspose2d-43           [-1, 64, 96, 96]          32,832\n",
      "      Concatlayer-44          [-1, 128, 96, 96]               0\n",
      "           Conv2d-45           [-1, 64, 96, 96]          73,792\n",
      "             ReLU-46           [-1, 64, 96, 96]               0\n",
      "           Conv2d-47           [-1, 64, 96, 96]          36,928\n",
      "             ReLU-48           [-1, 64, 96, 96]               0\n",
      "           Conv2d-49           [-1, 24, 96, 96]           1,560\n",
      "================================================================\n",
      "Total params: 31,033,240\n",
      "Trainable params: 31,033,240\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 97.73\n",
      "Params size (MB): 118.38\n",
      "Estimated Total Size (MB): 216.22\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "print(f\"Using Device {core.TorchDevice}\")\n",
    "\n",
    "torch.set_default_device(core.TorchDevice)\n",
    "\n",
    "model = UNET()\n",
    "\n",
    "print(model)\n",
    "\n",
    "summary(model, (3, core.ImageSize, core.ImageSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 50 loss: 0.0032501759612932803\n",
      "  batch 100 loss: 0.002879187394864857\n",
      "  batch 150 loss: 0.0028179553244262936\n",
      "  batch 200 loss: 0.0027906137704849245\n",
      "  batch 250 loss: 0.0028607686003670097\n",
      "  batch 300 loss: 0.0028244197461754082\n",
      "  batch 350 loss: 0.002854188485071063\n",
      "  batch 400 loss: 0.0027756344433873892\n",
      "  batch 450 loss: 0.0027345383446663618\n",
      "  batch 500 loss: 0.0027398083498701453\n",
      "  batch 550 loss: 0.002719235462136567\n",
      "  batch 600 loss: 0.0026829785853624342\n",
      "  batch 650 loss: 0.00269110984634608\n",
      "  batch 700 loss: 0.0026155268074944614\n",
      "  batch 750 loss: 0.0026371911820024254\n",
      "  batch 800 loss: 0.0026642365800216795\n",
      "  batch 850 loss: 0.002568009146489203\n",
      "  batch 900 loss: 0.0025156960031017662\n",
      "  batch 950 loss: 0.0025231396732851865\n",
      "  batch 1000 loss: 0.0024942302890121936\n",
      "  batch 1050 loss: 0.0025183442840352654\n",
      "  batch 1100 loss: 0.0025146150495857\n",
      "LOSS train 0.0025146150495857 valid 0.0022251636255532503\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 0.0024405482690781355\n",
      "  batch 100 loss: 0.002441625497303903\n",
      "  batch 150 loss: 0.002446298231370747\n",
      "  batch 200 loss: 0.0023593116062693297\n",
      "  batch 250 loss: 0.002342388576362282\n",
      "  batch 300 loss: 0.0023534430959261956\n",
      "  batch 350 loss: 0.0023339882073923944\n",
      "  batch 400 loss: 0.0023220648407004775\n",
      "  batch 450 loss: 0.002271933958400041\n",
      "  batch 500 loss: 0.0022627772507257757\n",
      "  batch 550 loss: 0.002226612481754273\n",
      "  batch 600 loss: 0.0022728163143619896\n",
      "  batch 650 loss: 0.0022475192160345612\n",
      "  batch 700 loss: 0.0021904516220092774\n",
      "  batch 750 loss: 0.0022392511810176074\n",
      "  batch 800 loss: 0.002209168819244951\n",
      "  batch 850 loss: 0.002182888321112841\n",
      "  batch 900 loss: 0.002198209043126553\n",
      "  batch 950 loss: 0.0020848716283217075\n",
      "  batch 1000 loss: 0.0021498719323426486\n",
      "  batch 1050 loss: 0.00207254626089707\n",
      "  batch 1100 loss: 0.002129987427033484\n",
      "LOSS train 0.002129987427033484 valid 0.0019627620931714773\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.00206566343549639\n",
      "  batch 100 loss: 0.002072544633410871\n",
      "  batch 150 loss: 0.0020527798240073027\n",
      "  batch 200 loss: 0.0020197488041594626\n",
      "  batch 250 loss: 0.0020410592132247985\n",
      "  batch 300 loss: 0.0019988893694244327\n",
      "  batch 350 loss: 0.002037559296004474\n",
      "  batch 400 loss: 0.0020354769611731173\n",
      "  batch 450 loss: 0.0020354933966882528\n",
      "  batch 500 loss: 0.002058983785100281\n",
      "  batch 550 loss: 0.001901872956659645\n",
      "  batch 600 loss: 0.002087034701835364\n",
      "  batch 650 loss: 0.0020376913552172483\n",
      "  batch 700 loss: 0.0020471707126125693\n",
      "  batch 750 loss: 0.0020419041719287635\n",
      "  batch 800 loss: 0.0019966224976815283\n",
      "  batch 850 loss: 0.001974552518222481\n",
      "  batch 900 loss: 0.001979299334343523\n",
      "  batch 950 loss: 0.001984584436286241\n",
      "  batch 1000 loss: 0.0019789531943388283\n",
      "  batch 1050 loss: 0.0019839567015878857\n",
      "  batch 1100 loss: 0.001924141061026603\n",
      "LOSS train 0.001924141061026603 valid 0.0018846801249310374\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.0019394599716179074\n",
      "  batch 100 loss: 0.001854005411732942\n",
      "  batch 150 loss: 0.0018906700285151602\n",
      "  batch 200 loss: 0.0019012418575584888\n",
      "  batch 250 loss: 0.0018968244432471693\n",
      "  batch 300 loss: 0.0019227356021292508\n",
      "  batch 350 loss: 0.0019001201214268804\n",
      "  batch 400 loss: 0.001882431278936565\n",
      "  batch 450 loss: 0.001866872450336814\n",
      "  batch 500 loss: 0.0018758585071191193\n",
      "  batch 550 loss: 0.001904006307013333\n",
      "  batch 600 loss: 0.0018923459458164871\n",
      "  batch 650 loss: 0.0018363522412255407\n",
      "  batch 700 loss: 0.0018437521136365832\n",
      "  batch 750 loss: 0.001883256030268967\n",
      "  batch 800 loss: 0.001862518263515085\n",
      "  batch 850 loss: 0.0019171677436679601\n",
      "  batch 900 loss: 0.0018487712927162648\n",
      "  batch 950 loss: 0.001929130726493895\n",
      "  batch 1000 loss: 0.0018427222827449442\n",
      "  batch 1050 loss: 0.0018591238767839968\n",
      "  batch 1100 loss: 0.0018464024504646658\n",
      "LOSS train 0.0018464024504646658 valid 0.001856622751802206\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.0018068865314126014\n",
      "  batch 100 loss: 0.0017480688402429223\n",
      "  batch 150 loss: 0.0017871659388765692\n",
      "  batch 200 loss: 0.001824976543430239\n",
      "  batch 250 loss: 0.0018189249187707901\n",
      "  batch 300 loss: 0.0018141782819293439\n",
      "  batch 350 loss: 0.00177042348543182\n",
      "  batch 400 loss: 0.0017890402814373374\n",
      "  batch 450 loss: 0.0017906810087151826\n",
      "  batch 500 loss: 0.0018402953119948506\n",
      "  batch 550 loss: 0.0017655842727981508\n",
      "  batch 600 loss: 0.0018046591384336353\n",
      "  batch 650 loss: 0.0017683530505746602\n",
      "  batch 700 loss: 0.001793298521079123\n",
      "  batch 750 loss: 0.0018072966788895428\n",
      "  batch 800 loss: 0.0018225022568367421\n",
      "  batch 850 loss: 0.0018170007434673606\n",
      "  batch 900 loss: 0.0017940199072472751\n",
      "  batch 950 loss: 0.001809552500490099\n",
      "  batch 1000 loss: 0.0017299349466338753\n",
      "  batch 1050 loss: 0.0017550048674456775\n",
      "  batch 1100 loss: 0.0017956889234483242\n",
      "LOSS train 0.0017956889234483242 valid 0.001808825647458434\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.001716672705952078\n",
      "  batch 100 loss: 0.001738486485555768\n",
      "  batch 150 loss: 0.0017305994033813476\n",
      "  batch 200 loss: 0.0017030437081120909\n",
      "  batch 250 loss: 0.0017339561670087278\n",
      "  batch 300 loss: 0.0017606711760163307\n",
      "  batch 350 loss: 0.00172044369392097\n",
      "  batch 400 loss: 0.0017238763649947942\n",
      "  batch 450 loss: 0.0017260976089164615\n",
      "  batch 500 loss: 0.0017486347304657103\n",
      "  batch 550 loss: 0.001744922874495387\n",
      "  batch 600 loss: 0.001718919000122696\n",
      "  batch 650 loss: 0.0016959452093578875\n",
      "  batch 700 loss: 0.0017377657210454346\n",
      "  batch 750 loss: 0.0017658416857011616\n",
      "  batch 800 loss: 0.0017359641962684691\n",
      "  batch 850 loss: 0.0017354991496540606\n",
      "  batch 900 loss: 0.0017134803626686335\n",
      "  batch 950 loss: 0.0017182301147840918\n",
      "  batch 1000 loss: 0.0017666822462342679\n",
      "  batch 1050 loss: 0.001700641568750143\n",
      "  batch 1100 loss: 0.0017220242274925113\n",
      "LOSS train 0.0017220242274925113 valid 0.0017737046582624316\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.001736060359980911\n",
      "  batch 100 loss: 0.0016598830791190267\n",
      "  batch 150 loss: 0.0016489254822954534\n",
      "  batch 200 loss: 0.0016765162185765803\n",
      "  batch 250 loss: 0.0016920582042075694\n",
      "  batch 300 loss: 0.001662438597995788\n",
      "  batch 350 loss: 0.0016201751888729632\n",
      "  batch 400 loss: 0.0017032906855456531\n",
      "  batch 450 loss: 0.0016908227768726648\n",
      "  batch 500 loss: 0.0017340598232112826\n",
      "  batch 550 loss: 0.0016550050466321408\n",
      "  batch 600 loss: 0.0017794148600660264\n",
      "  batch 650 loss: 0.001694531438406557\n",
      "  batch 700 loss: 0.0017094159219413997\n",
      "  batch 750 loss: 0.0016640194598585368\n",
      "  batch 800 loss: 0.0016284490656107665\n",
      "  batch 850 loss: 0.0016534658009186387\n",
      "  batch 900 loss: 0.0016395463910885155\n",
      "  batch 950 loss: 0.0016818113392218948\n",
      "  batch 1000 loss: 0.0016705589951016009\n",
      "  batch 1050 loss: 0.0016310200560837984\n",
      "  batch 1100 loss: 0.0016622515674680472\n",
      "LOSS train 0.0016622515674680472 valid 0.001780250109732151\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.0016980479517951608\n",
      "  batch 100 loss: 0.0016320872027426958\n",
      "  batch 150 loss: 0.0016601832560263574\n",
      "  batch 200 loss: 0.0016481398744508625\n",
      "  batch 250 loss: 0.0015821455791592597\n",
      "  batch 300 loss: 0.0015907014813274145\n",
      "  batch 350 loss: 0.0016628548689186574\n",
      "  batch 400 loss: 0.001638302900828421\n",
      "  batch 450 loss: 0.0016266369773074985\n",
      "  batch 500 loss: 0.0015916913654655219\n",
      "  batch 550 loss: 0.0016331958887167274\n",
      "  batch 600 loss: 0.0016664193454198539\n",
      "  batch 650 loss: 0.0016570034087635577\n",
      "  batch 700 loss: 0.001667002213653177\n",
      "  batch 750 loss: 0.0016381581220775842\n",
      "  batch 800 loss: 0.0016876369528472423\n",
      "  batch 850 loss: 0.0016166374925523997\n",
      "  batch 900 loss: 0.0015959387784823776\n",
      "  batch 950 loss: 0.0016109470091760158\n",
      "  batch 1000 loss: 0.0016736611514352262\n",
      "  batch 1050 loss: 0.0016657419758848845\n",
      "  batch 1100 loss: 0.0016215852252207696\n",
      "LOSS train 0.0016215852252207696 valid 0.0017568968469277024\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.001589128647465259\n",
      "  batch 100 loss: 0.0015876797889359294\n",
      "  batch 150 loss: 0.0015759038901887834\n",
      "  batch 200 loss: 0.0015969185065478086\n",
      "  batch 250 loss: 0.001601266993675381\n",
      "  batch 300 loss: 0.0015533110569231213\n",
      "  batch 350 loss: 0.0015526150469668209\n",
      "  batch 400 loss: 0.0015905485302209854\n",
      "  batch 450 loss: 0.0016214313078671695\n",
      "  batch 500 loss: 0.0015552549855783581\n",
      "  batch 550 loss: 0.0016408958146348596\n",
      "  batch 600 loss: 0.00158882005373016\n",
      "  batch 650 loss: 0.001604750631377101\n",
      "  batch 700 loss: 0.001630424614995718\n",
      "  batch 750 loss: 0.0016126995836384594\n",
      "  batch 800 loss: 0.001650011078454554\n",
      "  batch 850 loss: 0.0015800538728944957\n",
      "  batch 900 loss: 0.0016515867318958044\n",
      "  batch 950 loss: 0.001613170993514359\n",
      "  batch 1000 loss: 0.0016386757884174585\n",
      "  batch 1050 loss: 0.001622712085954845\n",
      "  batch 1100 loss: 0.0016277828579768538\n",
      "LOSS train 0.0016277828579768538 valid 0.0017232532845810056\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.0015269229025579988\n",
      "  batch 100 loss: 0.0015532774268649518\n",
      "  batch 150 loss: 0.001535174623131752\n",
      "  batch 200 loss: 0.0015661216271109879\n",
      "  batch 250 loss: 0.0015779707464389503\n",
      "  batch 300 loss: 0.001560731278732419\n",
      "  batch 350 loss: 0.001537476780358702\n",
      "  batch 400 loss: 0.0015693112835288047\n",
      "  batch 450 loss: 0.0015756346099078656\n",
      "  batch 500 loss: 0.0015902572660706938\n",
      "  batch 550 loss: 0.0015747261350043118\n",
      "  batch 600 loss: 0.0015931095369160176\n",
      "  batch 650 loss: 0.00157040327321738\n",
      "  batch 700 loss: 0.001560595268383622\n",
      "  batch 750 loss: 0.0015893720323219896\n",
      "  batch 800 loss: 0.001558911914471537\n",
      "  batch 850 loss: 0.0015830596210435032\n",
      "  batch 900 loss: 0.0015728718205355108\n",
      "  batch 950 loss: 0.0015823221718892456\n",
      "  batch 1000 loss: 0.0015999551001004875\n",
      "  batch 1050 loss: 0.0016125819948501884\n",
      "  batch 1100 loss: 0.0015557145443744957\n",
      "LOSS train 0.0015557145443744957 valid 0.0017707657534629107\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.0015844438527710736\n",
      "  batch 100 loss: 0.0015518047800287605\n",
      "  batch 150 loss: 0.0015020395955070854\n",
      "  batch 200 loss: 0.0015195839083753527\n",
      "  batch 250 loss: 0.0015097989607602358\n",
      "  batch 300 loss: 0.0015313484566286206\n",
      "  batch 350 loss: 0.0015758567373268307\n",
      "  batch 400 loss: 0.0015394910029135644\n",
      "  batch 450 loss: 0.0015007814695127309\n",
      "  batch 500 loss: 0.0015536837209947406\n",
      "  batch 550 loss: 0.0015842649363912642\n",
      "  batch 600 loss: 0.001513052973896265\n",
      "  batch 650 loss: 0.0015824110037647187\n",
      "  batch 700 loss: 0.001600357003044337\n",
      "  batch 750 loss: 0.0015035794372670352\n",
      "  batch 800 loss: 0.0015359359444119037\n",
      "  batch 850 loss: 0.0015439881919883192\n",
      "  batch 900 loss: 0.0015780477644875646\n",
      "  batch 950 loss: 0.001530391511041671\n",
      "  batch 1000 loss: 0.0015363121801055969\n",
      "  batch 1050 loss: 0.0015963365533389151\n",
      "  batch 1100 loss: 0.0015951413265429437\n",
      "LOSS train 0.0015951413265429437 valid 0.001750911003910005\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.0014951870893128217\n",
      "  batch 100 loss: 0.0014968394197057934\n",
      "  batch 150 loss: 0.0014824750018306077\n",
      "  batch 200 loss: 0.0014323870558291673\n",
      "  batch 250 loss: 0.0014936605026014149\n",
      "  batch 300 loss: 0.0014465193380601704\n",
      "  batch 350 loss: 0.0014848730457015335\n",
      "  batch 400 loss: 0.0015056906966492533\n",
      "  batch 450 loss: 0.0015267486940138042\n",
      "  batch 500 loss: 0.0015457770600914955\n",
      "  batch 550 loss: 0.0015479272976517678\n",
      "  batch 600 loss: 0.0015401350893080234\n",
      "  batch 650 loss: 0.001566093536093831\n",
      "  batch 700 loss: 0.0015239795413799584\n",
      "  batch 750 loss: 0.0015250660455785692\n",
      "  batch 800 loss: 0.0015414117346517742\n",
      "  batch 850 loss: 0.0015481516416184604\n",
      "  batch 900 loss: 0.0014846782642416656\n",
      "  batch 950 loss: 0.0015213121147826315\n",
      "  batch 1000 loss: 0.0015448611811734736\n",
      "  batch 1050 loss: 0.0015587761951610447\n",
      "  batch 1100 loss: 0.0015690815704874695\n",
      "LOSS train 0.0015690815704874695 valid 0.0017611619550734758\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.0014483606535941362\n",
      "  batch 100 loss: 0.0014781840890645982\n",
      "  batch 150 loss: 0.0014661617320962251\n",
      "  batch 200 loss: 0.0014648134890012444\n",
      "  batch 250 loss: 0.001475680680014193\n",
      "  batch 300 loss: 0.0014739809930324555\n",
      "  batch 350 loss: 0.0014675899897702038\n",
      "  batch 400 loss: 0.0014840977150015534\n",
      "  batch 450 loss: 0.0014897265192121267\n",
      "  batch 500 loss: 0.0014784236648119986\n",
      "  batch 550 loss: 0.0014988786075264215\n",
      "  batch 600 loss: 0.0015650781895965337\n",
      "  batch 650 loss: 0.0014749328908510507\n",
      "  batch 700 loss: 0.0015363359288312495\n",
      "  batch 750 loss: 0.001560002991463989\n",
      "  batch 800 loss: 0.001500799034256488\n",
      "  batch 850 loss: 0.0015102653577923775\n",
      "  batch 900 loss: 0.0015096932044252754\n",
      "  batch 950 loss: 0.0014880089741200208\n",
      "  batch 1000 loss: 0.001521457394119352\n",
      "  batch 1050 loss: 0.0015335398563183845\n",
      "  batch 1100 loss: 0.0015381912514567375\n",
      "LOSS train 0.0015381912514567375 valid 0.0017812659498304129\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.0013993401848711074\n",
      "  batch 100 loss: 0.001418962199240923\n",
      "  batch 150 loss: 0.0014696716098114848\n",
      "  batch 200 loss: 0.00147524309810251\n",
      "  batch 250 loss: 0.001487617543898523\n",
      "  batch 300 loss: 0.001488449501339346\n",
      "  batch 350 loss: 0.0014720338257029652\n",
      "  batch 400 loss: 0.0014559622353408485\n",
      "  batch 450 loss: 0.0014795895386487245\n",
      "  batch 500 loss: 0.001488133433740586\n",
      "  batch 550 loss: 0.0015379453147761524\n",
      "  batch 600 loss: 0.0014763921382836998\n",
      "  batch 650 loss: 0.0015160094061866403\n",
      "  batch 700 loss: 0.001512059923261404\n",
      "  batch 750 loss: 0.0015015406417660416\n",
      "  batch 800 loss: 0.0015212525497190655\n",
      "  batch 850 loss: 0.001546289015095681\n",
      "  batch 900 loss: 0.0015049303276464343\n",
      "  batch 950 loss: 0.001485597169958055\n",
      "  batch 1000 loss: 0.0014827464171685279\n",
      "  batch 1050 loss: 0.0014790879003703595\n",
      "  batch 1100 loss: 0.0014640557812526823\n",
      "LOSS train 0.0014640557812526823 valid 0.0017231084639206529\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.0013687077816575766\n",
      "  batch 100 loss: 0.0014238164434209465\n",
      "  batch 150 loss: 0.00141319977119565\n",
      "  batch 200 loss: 0.0014119216660037636\n",
      "  batch 250 loss: 0.0014507658663205803\n",
      "  batch 300 loss: 0.0014129452570341527\n",
      "  batch 350 loss: 0.0014388681389391421\n",
      "  batch 400 loss: 0.0014554700185544789\n",
      "  batch 450 loss: 0.0014381620404310524\n",
      "  batch 500 loss: 0.001469594466034323\n",
      "  batch 550 loss: 0.001448638360016048\n",
      "  batch 600 loss: 0.0014869650918990375\n",
      "  batch 650 loss: 0.001424173591658473\n",
      "  batch 700 loss: 0.0014852055720984937\n",
      "  batch 750 loss: 0.001498714517802\n",
      "  batch 800 loss: 0.001496414456050843\n",
      "  batch 850 loss: 0.0015065965708345175\n",
      "  batch 900 loss: 0.0014887383813038469\n",
      "  batch 950 loss: 0.0014777408284135163\n",
      "  batch 1000 loss: 0.0014329160354100167\n",
      "  batch 1050 loss: 0.0014620947395451367\n",
      "  batch 1100 loss: 0.0014801391516812146\n",
      "LOSS train 0.0014801391516812146 valid 0.001750426716171205\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.0014583661803044378\n",
      "  batch 100 loss: 0.0014653222868219018\n",
      "  batch 150 loss: 0.0014579651737585664\n",
      "  batch 200 loss: 0.0014507267344743013\n",
      "  batch 250 loss: 0.0014778587035834789\n",
      "  batch 300 loss: 0.00141941852401942\n",
      "  batch 350 loss: 0.001438397878082469\n",
      "  batch 400 loss: 0.0014009035658091307\n",
      "  batch 450 loss: 0.0014273219020105898\n",
      "  batch 500 loss: 0.0014554641186259687\n",
      "  batch 550 loss: 0.0014582403865642845\n",
      "  batch 600 loss: 0.0014591555576771499\n",
      "  batch 650 loss: 0.001393191039096564\n",
      "  batch 700 loss: 0.0014198447624221445\n",
      "  batch 750 loss: 0.0014516358589753509\n",
      "  batch 800 loss: 0.0014589635049924255\n",
      "  batch 850 loss: 0.0014366245828568935\n",
      "  batch 900 loss: 0.001416640318930149\n",
      "  batch 950 loss: 0.0014245281077455729\n",
      "  batch 1000 loss: 0.001504813141655177\n",
      "  batch 1050 loss: 0.0014432412665337324\n",
      "  batch 1100 loss: 0.0015045596822164952\n",
      "LOSS train 0.0015045596822164952 valid 0.001748752547428012\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.001363860305864364\n",
      "  batch 100 loss: 0.0014198382338508964\n",
      "  batch 150 loss: 0.001414381905924529\n",
      "  batch 200 loss: 0.0013877268019132317\n",
      "  batch 250 loss: 0.0013999833469279111\n",
      "  batch 300 loss: 0.0014396165101788938\n",
      "  batch 350 loss: 0.0014203366450965405\n",
      "  batch 400 loss: 0.001382379571441561\n",
      "  batch 450 loss: 0.0014558467199094593\n",
      "  batch 500 loss: 0.0014163765963166952\n",
      "  batch 550 loss: 0.0014032276789657772\n",
      "  batch 600 loss: 0.001410080089699477\n",
      "  batch 650 loss: 0.0014550659153610467\n",
      "  batch 700 loss: 0.001457205966580659\n",
      "  batch 750 loss: 0.001433316203765571\n",
      "  batch 800 loss: 0.0014742499659769236\n",
      "  batch 850 loss: 0.0014591333572752773\n",
      "  batch 900 loss: 0.0014323766529560089\n",
      "  batch 950 loss: 0.0014636115496978164\n",
      "  batch 1000 loss: 0.0014581938367336988\n",
      "  batch 1050 loss: 0.0014449433074332774\n",
      "  batch 1100 loss: 0.0014560672454535962\n",
      "LOSS train 0.0014560672454535962 valid 0.0017487078439444304\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.001417896025814116\n",
      "  batch 100 loss: 0.0014256596588529647\n",
      "  batch 150 loss: 0.0013908723648637534\n",
      "  batch 200 loss: 0.0013992849807254971\n",
      "  batch 250 loss: 0.00141583610791713\n",
      "  batch 300 loss: 0.001421904480084777\n",
      "  batch 350 loss: 0.001423524112906307\n",
      "  batch 400 loss: 0.0013878149492666126\n",
      "  batch 450 loss: 0.0014768303348682821\n",
      "  batch 500 loss: 0.0014449807070195675\n",
      "  batch 550 loss: 0.0014021279802545906\n",
      "  batch 600 loss: 0.001424039581324905\n",
      "  batch 650 loss: 0.0013960050512105227\n",
      "  batch 700 loss: 0.0014104477991349995\n",
      "  batch 750 loss: 0.0014266576338559389\n",
      "  batch 800 loss: 0.001425381861627102\n",
      "  batch 850 loss: 0.0014341928740032018\n",
      "  batch 900 loss: 0.0013964312896132469\n",
      "  batch 950 loss: 0.0013872465514577926\n",
      "  batch 1000 loss: 0.001427763041574508\n",
      "  batch 1050 loss: 0.0014366998779587448\n",
      "  batch 1100 loss: 0.001464920190628618\n",
      "LOSS train 0.001464920190628618 valid 0.0017487877048552036\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.0013248348073102534\n",
      "  batch 100 loss: 0.0013472387730143964\n",
      "  batch 150 loss: 0.0013503977726213633\n",
      "  batch 200 loss: 0.0013972450396977365\n",
      "  batch 250 loss: 0.001370953645091504\n",
      "  batch 300 loss: 0.0013761142455041408\n",
      "  batch 350 loss: 0.0013635999173857271\n",
      "  batch 400 loss: 0.0013992108823731543\n",
      "  batch 450 loss: 0.0013992991135455667\n",
      "  batch 500 loss: 0.0013967928243800998\n",
      "  batch 550 loss: 0.0014322867547161876\n",
      "  batch 600 loss: 0.0014445257023908199\n",
      "  batch 650 loss: 0.0014530582493171096\n",
      "  batch 700 loss: 0.0013568815751932562\n",
      "  batch 750 loss: 0.0014443975640460848\n",
      "  batch 800 loss: 0.001402929227333516\n",
      "  batch 850 loss: 0.0013379675569012762\n",
      "  batch 900 loss: 0.0014396141550969332\n",
      "  batch 950 loss: 0.001419110286515206\n",
      "  batch 1000 loss: 0.00138496701605618\n",
      "  batch 1050 loss: 0.0014458666346035898\n",
      "  batch 1100 loss: 0.001444194505456835\n",
      "LOSS train 0.001444194505456835 valid 0.0017635452095419168\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.0013559767836704851\n",
      "  batch 100 loss: 0.0013730094605125488\n",
      "  batch 150 loss: 0.0013683711737394334\n",
      "  batch 200 loss: 0.0013357643003109843\n",
      "  batch 250 loss: 0.0013488630088977515\n",
      "  batch 300 loss: 0.0013344753929413855\n",
      "  batch 350 loss: 0.0013428955234121532\n",
      "  batch 400 loss: 0.0013797394675202667\n",
      "  batch 450 loss: 0.001336278128437698\n",
      "  batch 500 loss: 0.0013652011740487068\n",
      "  batch 550 loss: 0.0013678504736162722\n",
      "  batch 600 loss: 0.00135579536203295\n",
      "  batch 650 loss: 0.0014351029763929545\n",
      "  batch 700 loss: 0.0013907896424643696\n",
      "  batch 750 loss: 0.0014028308098204434\n",
      "  batch 800 loss: 0.0013829109398648142\n",
      "  batch 850 loss: 0.0014279779861681163\n",
      "  batch 900 loss: 0.0014330296032130719\n",
      "  batch 950 loss: 0.001365872963797301\n",
      "  batch 1000 loss: 0.0014077250636182726\n",
      "  batch 1050 loss: 0.0013630756526254117\n",
      "  batch 1100 loss: 0.0014348701364360749\n",
      "LOSS train 0.0014348701364360749 valid 0.00175318808760494\n",
      "EPOCH 21:\n",
      "  batch 50 loss: 0.0012767963216174395\n",
      "  batch 100 loss: 0.0013461754028685391\n",
      "  batch 150 loss: 0.0013298195460811257\n",
      "  batch 200 loss: 0.001309832436963916\n",
      "  batch 250 loss: 0.0013840547180734575\n",
      "  batch 300 loss: 0.001357193326111883\n",
      "  batch 350 loss: 0.0013543379516340792\n",
      "  batch 400 loss: 0.001290839477442205\n",
      "  batch 450 loss: 0.0013458108669146896\n",
      "  batch 500 loss: 0.001344481974374503\n",
      "  batch 550 loss: 0.0013861891394481062\n",
      "  batch 600 loss: 0.0013416835910174995\n",
      "  batch 650 loss: 0.0013841762905940414\n",
      "  batch 700 loss: 0.0013687017234042286\n",
      "  batch 750 loss: 0.00137704893713817\n",
      "  batch 800 loss: 0.0014186710841022432\n",
      "  batch 850 loss: 0.0014634448615834118\n",
      "  batch 900 loss: 0.0013931030966341496\n",
      "  batch 950 loss: 0.001376731868367642\n",
      "  batch 1000 loss: 0.0013846431183628738\n",
      "  batch 1050 loss: 0.0013753131870180369\n",
      "  batch 1100 loss: 0.0013659962266683579\n",
      "LOSS train 0.0013659962266683579 valid 0.0017527458257973194\n",
      "EPOCH 22:\n",
      "  batch 50 loss: 0.0013284454052336515\n",
      "  batch 100 loss: 0.0012922137009445578\n",
      "  batch 150 loss: 0.001349496643524617\n",
      "  batch 200 loss: 0.0012669053568970412\n",
      "  batch 250 loss: 0.0013403526169713587\n",
      "  batch 300 loss: 0.001357363301794976\n",
      "  batch 350 loss: 0.0013370596256572754\n",
      "  batch 400 loss: 0.0013309630332514644\n",
      "  batch 450 loss: 0.001365161850117147\n",
      "  batch 500 loss: 0.0012793757161125542\n",
      "  batch 550 loss: 0.0013516628404613585\n",
      "  batch 600 loss: 0.0013191854115575552\n",
      "  batch 650 loss: 0.001386751369573176\n",
      "  batch 700 loss: 0.0013842310081236065\n",
      "  batch 750 loss: 0.0013440298405475914\n",
      "  batch 800 loss: 0.0013819191721268\n",
      "  batch 850 loss: 0.0013799357996322214\n",
      "  batch 900 loss: 0.001402596381958574\n",
      "  batch 950 loss: 0.0013796719210222364\n",
      "  batch 1000 loss: 0.0013567841029725968\n",
      "  batch 1050 loss: 0.0014194751693867146\n",
      "  batch 1100 loss: 0.0013844430912286043\n",
      "LOSS train 0.0013844430912286043 valid 0.0017181448638439178\n",
      "EPOCH 23:\n",
      "  batch 50 loss: 0.0013409328809939325\n",
      "  batch 100 loss: 0.0012997126206755639\n",
      "  batch 150 loss: 0.001275120535865426\n",
      "  batch 200 loss: 0.0013389162765815854\n",
      "  batch 250 loss: 0.001339368934277445\n",
      "  batch 300 loss: 0.001349277887493372\n",
      "  batch 350 loss: 0.0013680469314567745\n",
      "  batch 400 loss: 0.0013482436956837773\n",
      "  batch 450 loss: 0.001342656034976244\n",
      "  batch 500 loss: 0.0013607455557212234\n",
      "  batch 550 loss: 0.0013754358259029686\n",
      "  batch 600 loss: 0.0013325748732313514\n",
      "  batch 650 loss: 0.001303080691723153\n",
      "  batch 700 loss: 0.00132531123701483\n",
      "  batch 750 loss: 0.0013601065543480217\n",
      "  batch 800 loss: 0.0013247787521686405\n",
      "  batch 850 loss: 0.0013847351586446165\n",
      "  batch 900 loss: 0.0013753875717520713\n",
      "  batch 950 loss: 0.001347880456596613\n",
      "  batch 1000 loss: 0.0013632337329909205\n",
      "  batch 1050 loss: 0.0013464957429096102\n",
      "  batch 1100 loss: 0.0014050879725255071\n",
      "LOSS train 0.0014050879725255071 valid 0.0017717813607305288\n",
      "EPOCH 24:\n",
      "  batch 50 loss: 0.0012966330512426793\n",
      "  batch 100 loss: 0.0012519201810937376\n",
      "  batch 150 loss: 0.0012866092822514474\n",
      "  batch 200 loss: 0.0013141515804454683\n",
      "  batch 250 loss: 0.001291745046619326\n",
      "  batch 300 loss: 0.0012944246153347193\n",
      "  batch 350 loss: 0.001284085880033672\n",
      "  batch 400 loss: 0.0013263686071150005\n",
      "  batch 450 loss: 0.0012856378813739867\n",
      "  batch 500 loss: 0.0013308498868718744\n",
      "  batch 550 loss: 0.0013354220474138855\n",
      "  batch 600 loss: 0.0013109063310548664\n",
      "  batch 650 loss: 0.0013249106728471815\n",
      "  batch 700 loss: 0.001364615294151008\n",
      "  batch 750 loss: 0.0012957554263994097\n",
      "  batch 800 loss: 0.001319221192970872\n",
      "  batch 850 loss: 0.0013401444582268596\n",
      "  batch 900 loss: 0.001332197766751051\n",
      "  batch 950 loss: 0.0013370502775069326\n",
      "  batch 1000 loss: 0.0013592927833087742\n",
      "  batch 1050 loss: 0.0013388191699050366\n",
      "  batch 1100 loss: 0.0013053833309095353\n",
      "LOSS train 0.0013053833309095353 valid 0.00174052978400141\n",
      "EPOCH 25:\n",
      "  batch 50 loss: 0.0012507490743882954\n",
      "  batch 100 loss: 0.0012798339431174099\n",
      "  batch 150 loss: 0.001256787816528231\n",
      "  batch 200 loss: 0.0012702662602532655\n",
      "  batch 250 loss: 0.0012828334397636354\n",
      "  batch 300 loss: 0.0013019993831403552\n",
      "  batch 350 loss: 0.0013033388287294655\n",
      "  batch 400 loss: 0.0012570047669578344\n",
      "  batch 450 loss: 0.001356563065201044\n",
      "  batch 500 loss: 0.0012994643475394696\n",
      "  batch 550 loss: 0.0013309033052064478\n",
      "  batch 600 loss: 0.0012943795509636402\n",
      "  batch 650 loss: 0.0013744133710861207\n",
      "  batch 700 loss: 0.00132395627675578\n",
      "  batch 750 loss: 0.0013944985903799533\n",
      "  batch 800 loss: 0.0013343880278989674\n",
      "  batch 850 loss: 0.0013086654129438103\n",
      "  batch 900 loss: 0.0013073433528188617\n",
      "  batch 950 loss: 0.0013288017036393286\n",
      "  batch 1000 loss: 0.001332740888465196\n",
      "  batch 1050 loss: 0.0013110044947825372\n",
      "  batch 1100 loss: 0.0013453509949613363\n",
      "LOSS train 0.0013453509949613363 valid 0.0017618249403312802\n",
      "EPOCH 26:\n",
      "  batch 50 loss: 0.001295513950753957\n",
      "  batch 100 loss: 0.0012380262417718768\n",
      "  batch 150 loss: 0.0012710081262048334\n",
      "  batch 200 loss: 0.0012852739356458186\n",
      "  batch 250 loss: 0.001263458610046655\n",
      "  batch 300 loss: 0.001252556974068284\n",
      "  batch 350 loss: 0.0013199557363986968\n",
      "  batch 400 loss: 0.001274663615040481\n",
      "  batch 450 loss: 0.0012421523360535503\n",
      "  batch 500 loss: 0.001262430278584361\n",
      "  batch 550 loss: 0.0013294594641774893\n",
      "  batch 600 loss: 0.0012453430774621666\n",
      "  batch 650 loss: 0.0013087633461691438\n",
      "  batch 700 loss: 0.0013195999048184602\n",
      "  batch 750 loss: 0.0013014121050946414\n",
      "  batch 800 loss: 0.0012891076225787401\n",
      "  batch 850 loss: 0.0012847737688571215\n",
      "  batch 900 loss: 0.0013565914682112635\n",
      "  batch 950 loss: 0.0013498028926551343\n",
      "  batch 1000 loss: 0.0013680207543075086\n",
      "  batch 1050 loss: 0.0013701378600671887\n",
      "  batch 1100 loss: 0.0013672477519139647\n",
      "LOSS train 0.0013672477519139647 valid 0.001759934239089489\n",
      "EPOCH 27:\n",
      "  batch 50 loss: 0.0012482662894763052\n",
      "  batch 100 loss: 0.001251733099343255\n",
      "  batch 150 loss: 0.0012423822632990778\n",
      "  batch 200 loss: 0.0013080830383114517\n",
      "  batch 250 loss: 0.0012599668465554715\n",
      "  batch 300 loss: 0.0012608671898487955\n",
      "  batch 350 loss: 0.001237149591324851\n",
      "  batch 400 loss: 0.0012583256186917424\n",
      "  batch 450 loss: 0.0013026130746584385\n",
      "  batch 500 loss: 0.0012574263010174036\n",
      "  batch 550 loss: 0.0012599528499413282\n",
      "  batch 600 loss: 0.0012913230480626225\n",
      "  batch 650 loss: 0.001284712548367679\n",
      "  batch 700 loss: 0.0013081607199274003\n",
      "  batch 750 loss: 0.0012976100633386523\n",
      "  batch 800 loss: 0.0013842145958915353\n",
      "  batch 850 loss: 0.001303381189936772\n",
      "  batch 900 loss: 0.0013001738965976984\n",
      "  batch 950 loss: 0.0013143486296758055\n",
      "  batch 1000 loss: 0.0013370658922940493\n",
      "  batch 1050 loss: 0.0013223840529099106\n",
      "  batch 1100 loss: 0.001264771643327549\n",
      "LOSS train 0.001264771643327549 valid 0.0017415593611076474\n",
      "EPOCH 28:\n",
      "  batch 50 loss: 0.0011907638795673847\n",
      "  batch 100 loss: 0.0012479668983723967\n",
      "  batch 150 loss: 0.0011915737297385931\n",
      "  batch 200 loss: 0.0012458966777194292\n",
      "  batch 250 loss: 0.0012822344852611422\n",
      "  batch 300 loss: 0.0012925888411700725\n",
      "  batch 350 loss: 0.0012633870088029652\n",
      "  batch 400 loss: 0.001275698299286887\n",
      "  batch 450 loss: 0.0013052911637350918\n",
      "  batch 500 loss: 0.0012701517692767083\n",
      "  batch 550 loss: 0.0013148365565575659\n",
      "  batch 600 loss: 0.001259785924339667\n",
      "  batch 650 loss: 0.0012496982386801393\n",
      "  batch 700 loss: 0.0012612711067777128\n",
      "  batch 750 loss: 0.0012447042751591653\n",
      "  batch 800 loss: 0.0012364149373024701\n",
      "  batch 850 loss: 0.0012528659240342676\n",
      "  batch 900 loss: 0.0012969265528954565\n",
      "  batch 950 loss: 0.0012922920286655426\n",
      "  batch 1000 loss: 0.0012805881875101476\n",
      "  batch 1050 loss: 0.001278643028344959\n",
      "  batch 1100 loss: 0.001283722867956385\n",
      "LOSS train 0.001283722867956385 valid 0.0017839635256677866\n",
      "EPOCH 29:\n",
      "  batch 50 loss: 0.0012408073269762098\n",
      "  batch 100 loss: 0.00125077759148553\n",
      "  batch 150 loss: 0.0012085808697156607\n",
      "  batch 200 loss: 0.0012222781463060528\n",
      "  batch 250 loss: 0.0012212305061984807\n",
      "  batch 300 loss: 0.0012172877707052976\n",
      "  batch 350 loss: 0.001211795489070937\n",
      "  batch 400 loss: 0.001280189047101885\n",
      "  batch 450 loss: 0.0012296402011997998\n",
      "  batch 500 loss: 0.0012689231475815177\n",
      "  batch 550 loss: 0.0012890024483203889\n",
      "  batch 600 loss: 0.0012826051749289036\n",
      "  batch 650 loss: 0.001267529223114252\n",
      "  batch 700 loss: 0.0012387475022114813\n",
      "  batch 750 loss: 0.0012302278634160758\n",
      "  batch 800 loss: 0.001282234787940979\n",
      "  batch 850 loss: 0.0012769416242372244\n",
      "  batch 900 loss: 0.0013022188749164344\n",
      "  batch 950 loss: 0.0012744524818845094\n",
      "  batch 1000 loss: 0.0012702501972671598\n",
      "  batch 1050 loss: 0.0012626279995311051\n",
      "  batch 1100 loss: 0.0012903537054080516\n",
      "LOSS train 0.0012903537054080516 valid 0.0017823622329160571\n",
      "EPOCH 30:\n",
      "  batch 50 loss: 0.0012255627301055937\n",
      "  batch 100 loss: 0.001194540673168376\n",
      "  batch 150 loss: 0.0012757274613250046\n",
      "  batch 200 loss: 0.001234943603631109\n",
      "  batch 250 loss: 0.0012137701723258943\n",
      "  batch 300 loss: 0.0012610706710256636\n",
      "  batch 350 loss: 0.00126203729538247\n",
      "  batch 400 loss: 0.0012161121796816588\n",
      "  batch 450 loss: 0.0012092191784176976\n",
      "  batch 500 loss: 0.0012653678911738098\n",
      "  batch 550 loss: 0.0012562348158098758\n",
      "  batch 600 loss: 0.001223221787950024\n",
      "  batch 650 loss: 0.0012500859203282743\n",
      "  batch 700 loss: 0.0012833485857117921\n",
      "  batch 750 loss: 0.0012317325407639147\n",
      "  batch 800 loss: 0.001244265898130834\n",
      "  batch 850 loss: 0.0012402942031621933\n",
      "  batch 900 loss: 0.0012878213310614228\n",
      "  batch 950 loss: 0.0012328532233368606\n",
      "  batch 1000 loss: 0.0013267184293363243\n",
      "  batch 1050 loss: 0.0012664058560039849\n",
      "  batch 1100 loss: 0.0012395857076626271\n",
      "LOSS train 0.0012395857076626271 valid 0.0017760166665539145\n",
      "EPOCH 31:\n",
      "  batch 50 loss: 0.0012342651362996547\n",
      "  batch 100 loss: 0.001204579503973946\n",
      "  batch 150 loss: 0.0012342941621318459\n",
      "  batch 200 loss: 0.0011728527338709682\n",
      "  batch 250 loss: 0.0011997399199754\n",
      "  batch 300 loss: 0.0011771155335009097\n",
      "  batch 350 loss: 0.0012602920201607048\n",
      "  batch 400 loss: 0.00121762549970299\n",
      "  batch 450 loss: 0.001270683448528871\n",
      "  batch 500 loss: 0.001197134342510253\n",
      "  batch 550 loss: 0.001204008668428287\n",
      "  batch 600 loss: 0.0012133563577663153\n",
      "  batch 650 loss: 0.0012042578123509884\n",
      "  batch 700 loss: 0.001256085268687457\n",
      "  batch 750 loss: 0.0012403460894711315\n",
      "  batch 800 loss: 0.001228997679427266\n",
      "  batch 850 loss: 0.001225610168185085\n",
      "  batch 900 loss: 0.0012700134958140551\n",
      "  batch 950 loss: 0.0012454977130983026\n",
      "  batch 1000 loss: 0.0012680442549753935\n",
      "  batch 1050 loss: 0.0012861342704854905\n",
      "  batch 1100 loss: 0.0012887445965316146\n",
      "LOSS train 0.0012887445965316146 valid 0.0017789790872484446\n",
      "EPOCH 32:\n",
      "  batch 50 loss: 0.0011603792186360806\n",
      "  batch 100 loss: 0.001183510081609711\n",
      "  batch 150 loss: 0.0011617093067616225\n",
      "  batch 200 loss: 0.0012214220443274827\n",
      "  batch 250 loss: 0.0011567901668604462\n",
      "  batch 300 loss: 0.0011988819704856725\n",
      "  batch 350 loss: 0.0012161488353740423\n",
      "  batch 400 loss: 0.0012137746182270348\n",
      "  batch 450 loss: 0.0012445889529772102\n",
      "  batch 500 loss: 0.0012397890607826411\n",
      "  batch 550 loss: 0.0011755854089278727\n",
      "  batch 600 loss: 0.001220998305361718\n",
      "  batch 650 loss: 0.0012826425558887422\n",
      "  batch 700 loss: 0.0012828053440898657\n",
      "  batch 750 loss: 0.0012133501330390572\n",
      "  batch 800 loss: 0.0012846727285068483\n",
      "  batch 850 loss: 0.0012390618724748492\n",
      "  batch 900 loss: 0.0012439646059647202\n",
      "  batch 950 loss: 0.0012699091131798924\n",
      "  batch 1000 loss: 0.001204890797380358\n",
      "  batch 1050 loss: 0.0012602798058651387\n",
      "  batch 1100 loss: 0.0012574167991988361\n",
      "LOSS train 0.0012574167991988361 valid 0.001814310671761632\n",
      "EPOCH 33:\n",
      "  batch 50 loss: 0.001178187282057479\n",
      "  batch 100 loss: 0.0012021286354865878\n",
      "  batch 150 loss: 0.0011939058930147438\n",
      "  batch 200 loss: 0.0011757313064299523\n",
      "  batch 250 loss: 0.001158663348760456\n",
      "  batch 300 loss: 0.0011916855548042805\n",
      "  batch 350 loss: 0.001231611624825746\n",
      "  batch 400 loss: 0.0011524553783237934\n",
      "  batch 450 loss: 0.0012582421256229282\n",
      "  batch 500 loss: 0.001204325397266075\n",
      "  batch 550 loss: 0.0011847938515711575\n",
      "  batch 600 loss: 0.0012552153319120407\n",
      "  batch 650 loss: 0.0012058242119383066\n",
      "  batch 700 loss: 0.0012253984017297625\n",
      "  batch 750 loss: 0.0012121447967365384\n",
      "  batch 800 loss: 0.0012050725799053908\n",
      "  batch 850 loss: 0.0012288012367207557\n",
      "  batch 900 loss: 0.001200769345741719\n",
      "  batch 950 loss: 0.0011653954128269105\n",
      "  batch 1000 loss: 0.0012166614935267717\n",
      "  batch 1050 loss: 0.0012284513690974564\n",
      "  batch 1100 loss: 0.0012354681605938821\n",
      "LOSS train 0.0012354681605938821 valid 0.0017765009542927146\n",
      "EPOCH 34:\n",
      "  batch 50 loss: 0.0011139168730005622\n",
      "  batch 100 loss: 0.0011741781944874675\n",
      "  batch 150 loss: 0.0011186124465893954\n",
      "  batch 200 loss: 0.0011303622426930816\n",
      "  batch 250 loss: 0.0011490879696793854\n",
      "  batch 300 loss: 0.0011701596807688474\n",
      "  batch 350 loss: 0.0011556571861729027\n",
      "  batch 400 loss: 0.001215868827421218\n",
      "  batch 450 loss: 0.001177023007767275\n",
      "  batch 500 loss: 0.0011780984280630946\n",
      "  batch 550 loss: 0.0011580724327359348\n",
      "  batch 600 loss: 0.00118119818973355\n",
      "  batch 650 loss: 0.0012368585739750415\n",
      "  batch 700 loss: 0.0012437828374095262\n",
      "  batch 750 loss: 0.0012077351089101285\n",
      "  batch 800 loss: 0.0012187322159297764\n",
      "  batch 850 loss: 0.0011506940645631403\n",
      "  batch 900 loss: 0.001169239793671295\n",
      "  batch 950 loss: 0.001196869007544592\n",
      "  batch 1000 loss: 0.0012727506179362536\n",
      "  batch 1050 loss: 0.0012241449300199748\n",
      "  batch 1100 loss: 0.0012616430409252643\n",
      "LOSS train 0.0012616430409252643 valid 0.001803350867703557\n",
      "EPOCH 35:\n",
      "  batch 50 loss: 0.0011244654515758156\n",
      "  batch 100 loss: 0.0011598574055824429\n",
      "  batch 150 loss: 0.0011406837613321841\n",
      "  batch 200 loss: 0.0011435717844869942\n",
      "  batch 250 loss: 0.001148917693644762\n",
      "  batch 300 loss: 0.0011065259226597846\n",
      "  batch 350 loss: 0.001159665600862354\n",
      "  batch 400 loss: 0.0011196545523125677\n",
      "  batch 450 loss: 0.0011459358455613257\n",
      "  batch 500 loss: 0.0011886414082255214\n",
      "  batch 550 loss: 0.0011709193990100176\n",
      "  batch 600 loss: 0.0011920027039013803\n",
      "  batch 650 loss: 0.0012216502893716097\n",
      "  batch 700 loss: 0.0011625915591139345\n",
      "  batch 750 loss: 0.0011965105507988482\n",
      "  batch 800 loss: 0.0011807837977539748\n",
      "  batch 850 loss: 0.0012219148525036872\n",
      "  batch 900 loss: 0.0012259358877781778\n",
      "  batch 950 loss: 0.0011914787231944502\n",
      "  batch 1000 loss: 0.0012498792458791287\n",
      "  batch 1050 loss: 0.0012193941720761358\n",
      "  batch 1100 loss: 0.0012306161026936025\n",
      "LOSS train 0.0012306161026936025 valid 0.0017996248789131641\n",
      "EPOCH 36:\n",
      "  batch 50 loss: 0.0011500241048634052\n",
      "  batch 100 loss: 0.0011708936491049825\n",
      "  batch 150 loss: 0.001179983706679195\n",
      "  batch 200 loss: 0.0011643866763915867\n",
      "  batch 250 loss: 0.0011291139060631394\n",
      "  batch 300 loss: 0.0011756242031697184\n",
      "  batch 350 loss: 0.0011499712523072958\n",
      "  batch 400 loss: 0.0011578728817403317\n",
      "  batch 450 loss: 0.001172774131409824\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(FCN8_model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(FCN8_model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m core\u001b[38;5;241m.\u001b[39mtrain_model(model, optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, epochs\u001b[38;5;241m=\u001b[39mEPOCHS, run_dir\u001b[38;5;241m=\u001b[39mrun_dir, model_dir\u001b[38;5;241m=\u001b[39mmodel_dir)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\repos\\ECE579\\notebooks\\..\\src\\core.py:239\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, loss_fn, epochs, run_dir, model_dir)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m    238\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 239\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, optimizer, loss_fn, epoch_number, writer)\n\u001b[0;32m    242\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\repos\\ECE579\\notebooks\\..\\src\\core.py:206\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, loss_fn, epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m    205\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n\u001b[1;32m--> 206\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    208\u001b[0m \u001b[38;5;66;03m# Adjust learning weights\u001b[39;00m\n\u001b[0;32m    209\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\_tensor.py:513\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Computes the gradient of current tensor wrt graph leaves.\u001b[39;00m\n\u001b[0;32m    467\u001b[0m \n\u001b[0;32m    468\u001b[0m \u001b[38;5;124;03mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    510\u001b[0m \u001b[38;5;124;03m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[0;32m    511\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m    516\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    517\u001b[0m         gradient\u001b[38;5;241m=\u001b[39mgradient,\n\u001b[0;32m    518\u001b[0m         retain_graph\u001b[38;5;241m=\u001b[39mretain_graph,\n\u001b[0;32m    519\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[0;32m    522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\overrides.py:1604\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[1;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;66;03m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[0;32m   1602\u001b[0m     \u001b[38;5;66;03m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _pop_mode_temporarily() \u001b[38;5;28;01mas\u001b[39;00m mode:\n\u001b[1;32m-> 1604\u001b[0m         result \u001b[38;5;241m=\u001b[39m mode\u001b[38;5;241m.\u001b[39m__torch_function__(public_api, types, args, kwargs)\n\u001b[0;32m   1605\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[0;32m   1606\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = core.get_loss_fn()\n",
    "optimizer = core.get_optimizer(model)\n",
    "EPOCHS = 60\n",
    "cwd = os.getcwd()\n",
    "FCN8_model_dir = os.path.join(cwd, '../models', 'UNET_FINAL_NO_AUG')\n",
    "\n",
    "run_dir = os.path.join(FCN8_model_dir, 'run')\n",
    "model_dir = os.path.join(FCN8_model_dir, 'model')\n",
    "\n",
    "core.train_model(model, optimizer=optimizer, loss_fn=loss_fn, epochs=EPOCHS, run_dir=run_dir, model_dir=model_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece_579_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

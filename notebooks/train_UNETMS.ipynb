{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train FCN8 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys;sys.path.insert(0, '..')\n",
    "import os\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "from src.UNETMS import UNETMS\n",
    "import src.core as core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Device cuda\n",
      "UNETMS(\n",
      "  (conv_layer_1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_1_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (conv_layer_2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_2_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (conv_layer_3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_3_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (conv_layer_4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_4_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=(2, 2), stride=2, padding=0, dilation=1, ceil_mode=True)\n",
      "  )\n",
      "  (conv_layer_5): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_6_transposed): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cat_6): Concatlayer()\n",
      "  (conv_layer_6): Sequential(\n",
      "    (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_7_transposed): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cat_7): Concatlayer()\n",
      "  (conv_layer_7): Sequential(\n",
      "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_8_transposed): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cat_8): Concatlayer()\n",
      "  (conv_layer_8): Sequential(\n",
      "    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (conv_layer_9_transposed): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "  (cat_9): Concatlayer()\n",
      "  (conv_layer_9): Sequential(\n",
      "    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (cat_image_heatmap): Concatlayer()\n",
      "  (stage_2_conv_layer_1): Sequential(\n",
      "    (0): Conv2d(27, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (output): Conv2d(64, 24, kernel_size=(1, 1), stride=(1, 1), padding=valid)\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 64, 96, 96]           1,792\n",
      "              ReLU-2           [-1, 64, 96, 96]               0\n",
      "            Conv2d-3           [-1, 64, 96, 96]          36,928\n",
      "              ReLU-4           [-1, 64, 96, 96]               0\n",
      "         MaxPool2d-5           [-1, 64, 48, 48]               0\n",
      "            Conv2d-6          [-1, 128, 48, 48]          73,856\n",
      "              ReLU-7          [-1, 128, 48, 48]               0\n",
      "            Conv2d-8          [-1, 128, 48, 48]         147,584\n",
      "              ReLU-9          [-1, 128, 48, 48]               0\n",
      "        MaxPool2d-10          [-1, 128, 24, 24]               0\n",
      "           Conv2d-11          [-1, 256, 24, 24]         295,168\n",
      "             ReLU-12          [-1, 256, 24, 24]               0\n",
      "           Conv2d-13          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-14          [-1, 256, 24, 24]               0\n",
      "        MaxPool2d-15          [-1, 256, 12, 12]               0\n",
      "           Conv2d-16          [-1, 512, 12, 12]       1,180,160\n",
      "             ReLU-17          [-1, 512, 12, 12]               0\n",
      "           Conv2d-18          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-19          [-1, 512, 12, 12]               0\n",
      "        MaxPool2d-20            [-1, 512, 6, 6]               0\n",
      "           Conv2d-21           [-1, 1024, 6, 6]       4,719,616\n",
      "             ReLU-22           [-1, 1024, 6, 6]               0\n",
      "           Conv2d-23           [-1, 1024, 6, 6]       9,438,208\n",
      "             ReLU-24           [-1, 1024, 6, 6]               0\n",
      "  ConvTranspose2d-25          [-1, 512, 12, 12]       2,097,664\n",
      "      Concatlayer-26         [-1, 1024, 12, 12]               0\n",
      "           Conv2d-27          [-1, 512, 12, 12]       4,719,104\n",
      "             ReLU-28          [-1, 512, 12, 12]               0\n",
      "           Conv2d-29          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-30          [-1, 512, 12, 12]               0\n",
      "  ConvTranspose2d-31          [-1, 256, 24, 24]         524,544\n",
      "      Concatlayer-32          [-1, 512, 24, 24]               0\n",
      "           Conv2d-33          [-1, 256, 24, 24]       1,179,904\n",
      "             ReLU-34          [-1, 256, 24, 24]               0\n",
      "           Conv2d-35          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-36          [-1, 256, 24, 24]               0\n",
      "  ConvTranspose2d-37          [-1, 128, 48, 48]         131,200\n",
      "      Concatlayer-38          [-1, 256, 48, 48]               0\n",
      "           Conv2d-39          [-1, 128, 48, 48]         295,040\n",
      "             ReLU-40          [-1, 128, 48, 48]               0\n",
      "           Conv2d-41          [-1, 128, 48, 48]         147,584\n",
      "             ReLU-42          [-1, 128, 48, 48]               0\n",
      "  ConvTranspose2d-43           [-1, 64, 96, 96]          32,832\n",
      "      Concatlayer-44          [-1, 128, 96, 96]               0\n",
      "           Conv2d-45           [-1, 64, 96, 96]          73,792\n",
      "             ReLU-46           [-1, 64, 96, 96]               0\n",
      "           Conv2d-47           [-1, 64, 96, 96]          36,928\n",
      "             ReLU-48           [-1, 64, 96, 96]               0\n",
      "           Conv2d-49           [-1, 24, 96, 96]           1,560\n",
      "      Concatlayer-50           [-1, 27, 96, 96]               0\n",
      "           Conv2d-51           [-1, 64, 96, 96]          15,616\n",
      "             ReLU-52           [-1, 64, 96, 96]               0\n",
      "           Conv2d-53           [-1, 64, 96, 96]          36,928\n",
      "             ReLU-54           [-1, 64, 96, 96]               0\n",
      "        MaxPool2d-55           [-1, 64, 48, 48]               0\n",
      "           Conv2d-56          [-1, 128, 48, 48]          73,856\n",
      "             ReLU-57          [-1, 128, 48, 48]               0\n",
      "           Conv2d-58          [-1, 128, 48, 48]         147,584\n",
      "             ReLU-59          [-1, 128, 48, 48]               0\n",
      "        MaxPool2d-60          [-1, 128, 24, 24]               0\n",
      "           Conv2d-61          [-1, 256, 24, 24]         295,168\n",
      "             ReLU-62          [-1, 256, 24, 24]               0\n",
      "           Conv2d-63          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-64          [-1, 256, 24, 24]               0\n",
      "        MaxPool2d-65          [-1, 256, 12, 12]               0\n",
      "           Conv2d-66          [-1, 512, 12, 12]       1,180,160\n",
      "             ReLU-67          [-1, 512, 12, 12]               0\n",
      "           Conv2d-68          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-69          [-1, 512, 12, 12]               0\n",
      "        MaxPool2d-70            [-1, 512, 6, 6]               0\n",
      "           Conv2d-71           [-1, 1024, 6, 6]       4,719,616\n",
      "             ReLU-72           [-1, 1024, 6, 6]               0\n",
      "           Conv2d-73           [-1, 1024, 6, 6]       9,438,208\n",
      "             ReLU-74           [-1, 1024, 6, 6]               0\n",
      "  ConvTranspose2d-75          [-1, 512, 12, 12]       2,097,664\n",
      "      Concatlayer-76         [-1, 1024, 12, 12]               0\n",
      "           Conv2d-77          [-1, 512, 12, 12]       4,719,104\n",
      "             ReLU-78          [-1, 512, 12, 12]               0\n",
      "           Conv2d-79          [-1, 512, 12, 12]       2,359,808\n",
      "             ReLU-80          [-1, 512, 12, 12]               0\n",
      "  ConvTranspose2d-81          [-1, 256, 24, 24]         524,544\n",
      "      Concatlayer-82          [-1, 512, 24, 24]               0\n",
      "           Conv2d-83          [-1, 256, 24, 24]       1,179,904\n",
      "             ReLU-84          [-1, 256, 24, 24]               0\n",
      "           Conv2d-85          [-1, 256, 24, 24]         590,080\n",
      "             ReLU-86          [-1, 256, 24, 24]               0\n",
      "  ConvTranspose2d-87          [-1, 128, 48, 48]         131,200\n",
      "      Concatlayer-88          [-1, 256, 48, 48]               0\n",
      "           Conv2d-89          [-1, 128, 48, 48]         295,040\n",
      "             ReLU-90          [-1, 128, 48, 48]               0\n",
      "           Conv2d-91          [-1, 128, 48, 48]         147,584\n",
      "             ReLU-92          [-1, 128, 48, 48]               0\n",
      "  ConvTranspose2d-93           [-1, 64, 96, 96]          32,832\n",
      "      Concatlayer-94          [-1, 128, 96, 96]               0\n",
      "           Conv2d-95           [-1, 64, 96, 96]          73,792\n",
      "             ReLU-96           [-1, 64, 96, 96]               0\n",
      "           Conv2d-97           [-1, 64, 96, 96]          36,928\n",
      "             ReLU-98           [-1, 64, 96, 96]               0\n",
      "           Conv2d-99           [-1, 24, 96, 96]           1,560\n",
      "================================================================\n",
      "Total params: 62,080,304\n",
      "Trainable params: 62,080,304\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 197.37\n",
      "Params size (MB): 236.82\n",
      "Estimated Total Size (MB): 434.29\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "print(f\"Using Device {core.TorchDevice}\")\n",
    "\n",
    "torch.set_default_device(core.TorchDevice)\n",
    "\n",
    "model = UNETMS()\n",
    "\n",
    "print(model)\n",
    "\n",
    "summary(model, (3, core.ImageSize, core.ImageSize))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n",
      "  batch 50 loss: 0.003329213373363018\n",
      "  batch 100 loss: 0.002879894305951893\n",
      "  batch 150 loss: 0.0028106432594358923\n",
      "  batch 200 loss: 0.0027772008115425706\n",
      "  batch 250 loss: 0.00286855414044112\n",
      "  batch 300 loss: 0.002843018998391926\n",
      "  batch 350 loss: 0.0028755437396466733\n",
      "  batch 400 loss: 0.0028063365863636135\n",
      "  batch 450 loss: 0.002784322435036302\n",
      "  batch 500 loss: 0.0027877647522836922\n",
      "  batch 550 loss: 0.0027927850536070766\n",
      "  batch 600 loss: 0.0027748195501044394\n",
      "  batch 650 loss: 0.0027906665531918407\n",
      "  batch 700 loss: 0.002724836585111916\n",
      "  batch 750 loss: 0.002746470170095563\n",
      "  batch 800 loss: 0.002784632700495422\n",
      "  batch 850 loss: 0.0027085356460884212\n",
      "  batch 900 loss: 0.002652828157879412\n",
      "  batch 950 loss: 0.002671978576108813\n",
      "  batch 1000 loss: 0.0026670135091990234\n",
      "  batch 1050 loss: 0.0026951944921165703\n",
      "  batch 1100 loss: 0.002695696926675737\n",
      "LOSS train 0.002695696926675737 valid 0.0023936438374221325\n",
      "EPOCH 2:\n",
      "  batch 50 loss: 0.0026214551506564022\n",
      "  batch 100 loss: 0.002631280436180532\n",
      "  batch 150 loss: 0.0026715352851897477\n",
      "  batch 200 loss: 0.0025953641533851624\n",
      "  batch 250 loss: 0.002576821004040539\n",
      "  batch 300 loss: 0.002596337227150798\n",
      "  batch 350 loss: 0.002570276600308716\n",
      "  batch 400 loss: 0.002530307341367006\n",
      "  batch 450 loss: 0.0025106245698407292\n",
      "  batch 500 loss: 0.002476973976008594\n",
      "  batch 550 loss: 0.0024331809370778503\n",
      "  batch 600 loss: 0.0024955281568691135\n",
      "  batch 650 loss: 0.002467061476781964\n",
      "  batch 700 loss: 0.0024181464919820427\n",
      "  batch 750 loss: 0.0024414584087207912\n",
      "  batch 800 loss: 0.0024031759123317896\n",
      "  batch 850 loss: 0.0023958048154599965\n",
      "  batch 900 loss: 0.002390606130938977\n",
      "  batch 950 loss: 0.0022646818961948156\n",
      "  batch 1000 loss: 0.0023487611231394114\n",
      "  batch 1050 loss: 0.002272328743711114\n",
      "  batch 1100 loss: 0.0023055477207526566\n",
      "LOSS train 0.0023055477207526566 valid 0.002123181475326419\n",
      "EPOCH 3:\n",
      "  batch 50 loss: 0.0022553161927498875\n",
      "  batch 100 loss: 0.0022813084395602347\n",
      "  batch 150 loss: 0.0022897639218717814\n",
      "  batch 200 loss: 0.00222393257310614\n",
      "  batch 250 loss: 0.002252604998648167\n",
      "  batch 300 loss: 0.00218055565841496\n",
      "  batch 350 loss: 0.002192885568365455\n",
      "  batch 400 loss: 0.0022285370877943933\n",
      "  batch 450 loss: 0.002252521498594433\n",
      "  batch 500 loss: 0.002238792018033564\n",
      "  batch 550 loss: 0.002102507222443819\n",
      "  batch 600 loss: 0.0022767546004615726\n",
      "  batch 650 loss: 0.002203855246771127\n",
      "  batch 700 loss: 0.0022058421792462467\n",
      "  batch 750 loss: 0.0022170115588232876\n",
      "  batch 800 loss: 0.002181688433047384\n",
      "  batch 850 loss: 0.002166601417120546\n",
      "  batch 900 loss: 0.002170589577872306\n",
      "  batch 950 loss: 0.002141357508953661\n",
      "  batch 1000 loss: 0.002170999872032553\n",
      "  batch 1050 loss: 0.002195425354875624\n",
      "  batch 1100 loss: 0.0021066293842159213\n",
      "LOSS train 0.0021066293842159213 valid 0.00200977292843163\n",
      "EPOCH 4:\n",
      "  batch 50 loss: 0.002121023756917566\n",
      "  batch 100 loss: 0.002041422782931477\n",
      "  batch 150 loss: 0.0020769079308956863\n",
      "  batch 200 loss: 0.0021440967731177806\n",
      "  batch 250 loss: 0.002134085085708648\n",
      "  batch 300 loss: 0.0020627933088690044\n",
      "  batch 350 loss: 0.002081086803227663\n",
      "  batch 400 loss: 0.0020772497192956506\n",
      "  batch 450 loss: 0.0020542469224892556\n",
      "  batch 500 loss: 0.002069103044923395\n",
      "  batch 550 loss: 0.0020738136558793485\n",
      "  batch 600 loss: 0.002086229450069368\n",
      "  batch 650 loss: 0.0020632753730751574\n",
      "  batch 700 loss: 0.0020289919432252643\n",
      "  batch 750 loss: 0.002076242363546044\n",
      "  batch 800 loss: 0.0020279273693449796\n",
      "  batch 850 loss: 0.002073101804126054\n",
      "  batch 900 loss: 0.002028581614140421\n",
      "  batch 950 loss: 0.002060619823168963\n",
      "  batch 1000 loss: 0.001998914254363626\n",
      "  batch 1050 loss: 0.002039165566675365\n",
      "  batch 1100 loss: 0.0019783294457010924\n",
      "LOSS train 0.0019783294457010924 valid 0.00196815119124949\n",
      "EPOCH 5:\n",
      "  batch 50 loss: 0.0020336336689069866\n",
      "  batch 100 loss: 0.0019411405920982362\n",
      "  batch 150 loss: 0.0019903499679639934\n",
      "  batch 200 loss: 0.0020139300497248767\n",
      "  batch 250 loss: 0.0020461665419861676\n",
      "  batch 300 loss: 0.0019832109310664236\n",
      "  batch 350 loss: 0.0019608924025669694\n",
      "  batch 400 loss: 0.0020115219126455486\n",
      "  batch 450 loss: 0.0019527998054400086\n",
      "  batch 500 loss: 0.0020033797435462476\n",
      "  batch 550 loss: 0.001953806467354298\n",
      "  batch 600 loss: 0.0019821019237861036\n",
      "  batch 650 loss: 0.0019452585629187525\n",
      "  batch 700 loss: 0.001966775746550411\n",
      "  batch 750 loss: 0.001956363841891289\n",
      "  batch 800 loss: 0.0019623414520174264\n",
      "  batch 850 loss: 0.001995563572272658\n",
      "  batch 900 loss: 0.0019638412608765066\n",
      "  batch 950 loss: 0.0019952083937823774\n",
      "  batch 1000 loss: 0.0018974972562864423\n",
      "  batch 1050 loss: 0.00190713046817109\n",
      "  batch 1100 loss: 0.0019288834324106574\n",
      "LOSS train 0.0019288834324106574 valid 0.0019314008532091975\n",
      "EPOCH 6:\n",
      "  batch 50 loss: 0.0019100090954452752\n",
      "  batch 100 loss: 0.001942053756210953\n",
      "  batch 150 loss: 0.0018824272090569138\n",
      "  batch 200 loss: 0.0018850840511731803\n",
      "  batch 250 loss: 0.0019406658364459872\n",
      "  batch 300 loss: 0.0019330611336044966\n",
      "  batch 350 loss: 0.0018788599083200098\n",
      "  batch 400 loss: 0.0019009915250353514\n",
      "  batch 450 loss: 0.001886296970769763\n",
      "  batch 500 loss: 0.0019405948161147534\n",
      "  batch 550 loss: 0.001936951125971973\n",
      "  batch 600 loss: 0.0018695144611410796\n",
      "  batch 650 loss: 0.001862067391630262\n",
      "  batch 700 loss: 0.001916838667821139\n",
      "  batch 750 loss: 0.00193311425158754\n",
      "  batch 800 loss: 0.0019150832085870206\n",
      "  batch 850 loss: 0.0018995692813768984\n",
      "  batch 900 loss: 0.0018836038722656668\n",
      "  batch 950 loss: 0.0018781891535036266\n",
      "  batch 1000 loss: 0.001900744738522917\n",
      "  batch 1050 loss: 0.0018678539572283626\n",
      "  batch 1100 loss: 0.001878505942877382\n",
      "LOSS train 0.001878505942877382 valid 0.0018567172810435295\n",
      "EPOCH 7:\n",
      "  batch 50 loss: 0.0019077659095637501\n",
      "  batch 100 loss: 0.0018540773540735244\n",
      "  batch 150 loss: 0.001850927108898759\n",
      "  batch 200 loss: 0.0018912829970940948\n",
      "  batch 250 loss: 0.0019173465995118023\n",
      "  batch 300 loss: 0.001850650319829583\n",
      "  batch 350 loss: 0.0018019694811664522\n",
      "  batch 400 loss: 0.0018831782997585833\n",
      "  batch 450 loss: 0.001887123198248446\n",
      "  batch 500 loss: 0.0018933201162144543\n",
      "  batch 550 loss: 0.001808510017581284\n",
      "  batch 600 loss: 0.001964679907541722\n",
      "  batch 650 loss: 0.0018567079748027027\n",
      "  batch 700 loss: 0.001862432810012251\n",
      "  batch 750 loss: 0.0018397631193511187\n",
      "  batch 800 loss: 0.0018038003030233084\n",
      "  batch 850 loss: 0.0018165485956706106\n",
      "  batch 900 loss: 0.0018091720342636108\n",
      "  batch 950 loss: 0.0018063727556727827\n",
      "  batch 1000 loss: 0.0018315410078503193\n",
      "  batch 1050 loss: 0.0018227303866297007\n",
      "  batch 1100 loss: 0.0018172308709472418\n",
      "LOSS train 0.0018172308709472418 valid 0.0018395156366750598\n",
      "EPOCH 8:\n",
      "  batch 50 loss: 0.0018470440548844635\n",
      "  batch 100 loss: 0.0018450072011910379\n",
      "  batch 150 loss: 0.0018217648495920003\n",
      "  batch 200 loss: 0.0018259104783646762\n",
      "  batch 250 loss: 0.0017562260781414808\n",
      "  batch 300 loss: 0.0017650730069726705\n",
      "  batch 350 loss: 0.0018102332856506109\n",
      "  batch 400 loss: 0.001811639426741749\n",
      "  batch 450 loss: 0.001786351401824504\n",
      "  batch 500 loss: 0.0017419261578470468\n",
      "  batch 550 loss: 0.001781145459972322\n",
      "  batch 600 loss: 0.0018175672553479671\n",
      "  batch 650 loss: 0.0018352242116816341\n",
      "  batch 700 loss: 0.0018311987887136639\n",
      "  batch 750 loss: 0.001792182254139334\n",
      "  batch 800 loss: 0.0018463051482103766\n",
      "  batch 850 loss: 0.0017703964584507047\n",
      "  batch 900 loss: 0.0017677123169414699\n",
      "  batch 950 loss: 0.0017881690012291074\n",
      "  batch 1000 loss: 0.0018435250711627304\n",
      "  batch 1050 loss: 0.0018534273095428943\n",
      "  batch 1100 loss: 0.0018316415161825717\n",
      "LOSS train 0.0018316415161825717 valid 0.0018258604686707258\n",
      "EPOCH 9:\n",
      "  batch 50 loss: 0.0017526281555183233\n",
      "  batch 100 loss: 0.0017726198374293746\n",
      "  batch 150 loss: 0.0017471293266862632\n",
      "  batch 200 loss: 0.0018135934113524853\n",
      "  batch 250 loss: 0.0017620049905963241\n",
      "  batch 300 loss: 0.0017614654870703817\n",
      "  batch 350 loss: 0.0017242715531028806\n",
      "  batch 400 loss: 0.0017301048734225332\n",
      "  batch 450 loss: 0.001791114651132375\n",
      "  batch 500 loss: 0.0017354053491726517\n",
      "  batch 550 loss: 0.001810759617947042\n",
      "  batch 600 loss: 0.0017816992476582527\n",
      "  batch 650 loss: 0.0017703719926066696\n",
      "  batch 700 loss: 0.0017723886645399033\n",
      "  batch 750 loss: 0.00174722173018381\n",
      "  batch 800 loss: 0.0018131065275520087\n",
      "  batch 850 loss: 0.0017233021743595601\n",
      "  batch 900 loss: 0.001784994537010789\n",
      "  batch 950 loss: 0.0017778138420544564\n",
      "  batch 1000 loss: 0.001812960379756987\n",
      "  batch 1050 loss: 0.0017496333131566644\n",
      "  batch 1100 loss: 0.0017930989037267865\n",
      "LOSS train 0.0017930989037267865 valid 0.0017938707023859024\n",
      "EPOCH 10:\n",
      "  batch 50 loss: 0.0017238720669411124\n",
      "  batch 100 loss: 0.0017440209467895328\n",
      "  batch 150 loss: 0.0016929987235926093\n",
      "  batch 200 loss: 0.0017470469884574413\n",
      "  batch 250 loss: 0.0017713595228269696\n",
      "  batch 300 loss: 0.0017578817461617292\n",
      "  batch 350 loss: 0.00171935384394601\n",
      "  batch 400 loss: 0.0017519932286813855\n",
      "  batch 450 loss: 0.0017390297795645893\n",
      "  batch 500 loss: 0.0017371526430360973\n",
      "  batch 550 loss: 0.0017452682298608124\n",
      "  batch 600 loss: 0.0017418170371092856\n",
      "  batch 650 loss: 0.0017212360887788236\n",
      "  batch 700 loss: 0.001703218766488135\n",
      "  batch 750 loss: 0.0017586425924673676\n",
      "  batch 800 loss: 0.0017331926035694778\n",
      "  batch 850 loss: 0.0017243658076040447\n",
      "  batch 900 loss: 0.0017249022889882326\n",
      "  batch 950 loss: 0.0017438311129808425\n",
      "  batch 1000 loss: 0.0017528454028069972\n",
      "  batch 1050 loss: 0.0017852030391804874\n",
      "  batch 1100 loss: 0.0017448341217823327\n",
      "LOSS train 0.0017448341217823327 valid 0.0017998520052060485\n",
      "EPOCH 11:\n",
      "  batch 50 loss: 0.0017804434173740447\n",
      "  batch 100 loss: 0.0017306497041136026\n",
      "  batch 150 loss: 0.0016793703311122955\n",
      "  batch 200 loss: 0.001691005837637931\n",
      "  batch 250 loss: 0.0017034800536930561\n",
      "  batch 300 loss: 0.0016999982437118889\n",
      "  batch 350 loss: 0.0017081136908382178\n",
      "  batch 400 loss: 0.0017080134619027376\n",
      "  batch 450 loss: 0.001665154176298529\n",
      "  batch 500 loss: 0.0017101009842008352\n",
      "  batch 550 loss: 0.0017509642126969994\n",
      "  batch 600 loss: 0.0016797910537570714\n",
      "  batch 650 loss: 0.001752511174418032\n",
      "  batch 700 loss: 0.0018145126500166953\n",
      "  batch 750 loss: 0.0016767928027547896\n",
      "  batch 800 loss: 0.0016996172768995166\n",
      "  batch 850 loss: 0.0016945041390135885\n",
      "  batch 900 loss: 0.0017523468867875636\n",
      "  batch 950 loss: 0.001689108577556908\n",
      "  batch 1000 loss: 0.0017009311448782682\n",
      "  batch 1050 loss: 0.0017950346926227212\n",
      "  batch 1100 loss: 0.0017949951626360415\n",
      "LOSS train 0.0017949951626360415 valid 0.001899175113067031\n",
      "EPOCH 12:\n",
      "  batch 50 loss: 0.0017768052127212287\n",
      "  batch 100 loss: 0.0017715084599331022\n",
      "  batch 150 loss: 0.0017462909989990295\n",
      "  batch 200 loss: 0.0017176174907945097\n",
      "  batch 250 loss: 0.0017688750149682164\n",
      "  batch 300 loss: 0.0016754293185658753\n",
      "  batch 350 loss: 0.0017190168565139174\n",
      "  batch 400 loss: 0.001718939773272723\n",
      "  batch 450 loss: 0.0017124013742432\n",
      "  batch 500 loss: 0.0017331643239594996\n",
      "  batch 550 loss: 0.0017374978656880557\n",
      "  batch 600 loss: 0.001722549400292337\n",
      "  batch 650 loss: 0.0018005398358218372\n",
      "  batch 700 loss: 0.0017377505591139198\n",
      "  batch 750 loss: 0.0017614497453905642\n",
      "  batch 800 loss: 0.0017643751157447696\n",
      "  batch 850 loss: 0.0017547691217623652\n",
      "  batch 900 loss: 0.001717413878068328\n",
      "  batch 950 loss: 0.0017302286298945546\n",
      "  batch 1000 loss: 0.0017440250446088613\n",
      "  batch 1050 loss: 0.0017628124076873063\n",
      "  batch 1100 loss: 0.0017110817646607756\n",
      "LOSS train 0.0017110817646607756 valid 0.001822644262574613\n",
      "EPOCH 13:\n",
      "  batch 50 loss: 0.001659764691721648\n",
      "  batch 100 loss: 0.0016668776026926935\n",
      "  batch 150 loss: 0.0016644502943381667\n",
      "  batch 200 loss: 0.0016808939189650119\n",
      "  batch 250 loss: 0.0016667288029566408\n",
      "  batch 300 loss: 0.0016671400750055908\n",
      "  batch 350 loss: 0.0016747984709218145\n",
      "  batch 400 loss: 0.0017117594741284847\n",
      "  batch 450 loss: 0.0016936079901643097\n",
      "  batch 500 loss: 0.0016613379842601716\n",
      "  batch 550 loss: 0.0016720847017131747\n",
      "  batch 600 loss: 0.001750406411010772\n",
      "  batch 650 loss: 0.001671192841604352\n",
      "  batch 700 loss: 0.0017262795171700419\n",
      "  batch 750 loss: 0.0017378074885345996\n",
      "  batch 800 loss: 0.00167563836555928\n",
      "  batch 850 loss: 0.0017115965392440558\n",
      "  batch 900 loss: 0.0017256522621028126\n",
      "  batch 950 loss: 0.0016900850390084087\n",
      "  batch 1000 loss: 0.0017038339702412485\n",
      "  batch 1050 loss: 0.0017135473689995706\n",
      "  batch 1100 loss: 0.0017196137923747301\n",
      "LOSS train 0.0017196137923747301 valid 0.0018235392635688186\n",
      "EPOCH 14:\n",
      "  batch 50 loss: 0.0015819302434101701\n",
      "  batch 100 loss: 0.0016398918675258757\n",
      "  batch 150 loss: 0.001632225641515106\n",
      "  batch 200 loss: 0.0016653604293242097\n",
      "  batch 250 loss: 0.0016627354198135436\n",
      "  batch 300 loss: 0.0016536029847338796\n",
      "  batch 350 loss: 0.0016633653547614812\n",
      "  batch 400 loss: 0.0016238855151459574\n",
      "  batch 450 loss: 0.0016787477605976165\n",
      "  batch 500 loss: 0.0017000104673206807\n",
      "  batch 550 loss: 0.0017096762009896338\n",
      "  batch 600 loss: 0.0016316445870324969\n",
      "  batch 650 loss: 0.001685885456390679\n",
      "  batch 700 loss: 0.0017080334457568824\n",
      "  batch 750 loss: 0.0016640485706739129\n",
      "  batch 800 loss: 0.001702754208818078\n",
      "  batch 850 loss: 0.0017165144300088286\n",
      "  batch 900 loss: 0.001715668688993901\n",
      "  batch 950 loss: 0.0017315291962586342\n",
      "  batch 1000 loss: 0.0017669358849525451\n",
      "  batch 1050 loss: 0.0017122169048525394\n",
      "  batch 1100 loss: 0.0016903249267488719\n",
      "LOSS train 0.0016903249267488719 valid 0.0018031873041763902\n",
      "EPOCH 15:\n",
      "  batch 50 loss: 0.0016013873065821827\n",
      "  batch 100 loss: 0.0016599757480435073\n",
      "  batch 150 loss: 0.0015980764362029732\n",
      "  batch 200 loss: 0.0016039168555289508\n",
      "  batch 250 loss: 0.0016601207409985364\n",
      "  batch 300 loss: 0.0016466093203052878\n",
      "  batch 350 loss: 0.001652673517819494\n",
      "  batch 400 loss: 0.0016849619895219802\n",
      "  batch 450 loss: 0.0016486285137943923\n",
      "  batch 500 loss: 0.0016652802703902125\n",
      "  batch 550 loss: 0.001693105932790786\n",
      "  batch 600 loss: 0.001700430498458445\n",
      "  batch 650 loss: 0.0016259589698165654\n",
      "  batch 700 loss: 0.0016587382066063582\n",
      "  batch 750 loss: 0.0016984398220665753\n",
      "  batch 800 loss: 0.0016845402657054364\n",
      "  batch 850 loss: 0.0016966318082995712\n",
      "  batch 900 loss: 0.001668947790749371\n",
      "  batch 950 loss: 0.001670854529365897\n",
      "  batch 1000 loss: 0.0016141028446145355\n",
      "  batch 1050 loss: 0.0016173938568681478\n",
      "  batch 1100 loss: 0.0016498154937289655\n",
      "LOSS train 0.0016498154937289655 valid 0.0018215300515294075\n",
      "EPOCH 16:\n",
      "  batch 50 loss: 0.0016187806404195726\n",
      "  batch 100 loss: 0.00164516965392977\n",
      "  batch 150 loss: 0.0016381660848855972\n",
      "  batch 200 loss: 0.0016025729663670064\n",
      "  batch 250 loss: 0.0016516736592166125\n",
      "  batch 300 loss: 0.0016084873769432306\n",
      "  batch 350 loss: 0.0016033492982387542\n",
      "  batch 400 loss: 0.001579485279507935\n",
      "  batch 450 loss: 0.0016273638815619052\n",
      "  batch 500 loss: 0.0016688558366149664\n",
      "  batch 550 loss: 0.0016220997623167931\n",
      "  batch 600 loss: 0.0016557965544052422\n",
      "  batch 650 loss: 0.0016025502514094114\n",
      "  batch 700 loss: 0.001627127663232386\n",
      "  batch 750 loss: 0.0016518409946002066\n",
      "  batch 800 loss: 0.001657796180807054\n",
      "  batch 850 loss: 0.0016385762067511677\n",
      "  batch 900 loss: 0.0016085003386251628\n",
      "  batch 950 loss: 0.0016283106664195656\n",
      "  batch 1000 loss: 0.0016636980790644884\n",
      "  batch 1050 loss: 0.001608928730711341\n",
      "  batch 1100 loss: 0.0016701904241926968\n",
      "LOSS train 0.0016701904241926968 valid 0.0017749617109075189\n",
      "EPOCH 17:\n",
      "  batch 50 loss: 0.0015368048590607942\n",
      "  batch 100 loss: 0.001565537948627025\n",
      "  batch 150 loss: 0.0015743629354983569\n",
      "  batch 200 loss: 0.0015695510013028979\n",
      "  batch 250 loss: 0.0015716643282212316\n",
      "  batch 300 loss: 0.0015908904932439326\n",
      "  batch 350 loss: 0.0016048022755421698\n",
      "  batch 400 loss: 0.0015925900964066386\n",
      "  batch 450 loss: 0.001685742421541363\n",
      "  batch 500 loss: 0.0016209479584358632\n",
      "  batch 550 loss: 0.0015900860563851894\n",
      "  batch 600 loss: 0.0015959465107880533\n",
      "  batch 650 loss: 0.001662185729946941\n",
      "  batch 700 loss: 0.0016323824762366711\n",
      "  batch 750 loss: 0.0016308414819650352\n",
      "  batch 800 loss: 0.0016460620029829442\n",
      "  batch 850 loss: 0.0016163627803325654\n",
      "  batch 900 loss: 0.0016015142644755541\n",
      "  batch 950 loss: 0.001619037794880569\n",
      "  batch 1000 loss: 0.0016448464943096043\n",
      "  batch 1050 loss: 0.0016155597544275223\n",
      "  batch 1100 loss: 0.001668097365181893\n",
      "LOSS train 0.001668097365181893 valid 0.0018071840750053525\n",
      "EPOCH 18:\n",
      "  batch 50 loss: 0.0015292337932623923\n",
      "  batch 100 loss: 0.001534812692552805\n",
      "  batch 150 loss: 0.0015665867784991861\n",
      "  batch 200 loss: 0.0015849982690997423\n",
      "  batch 250 loss: 0.0015344424336217345\n",
      "  batch 300 loss: 0.0016097573423758148\n",
      "  batch 350 loss: 0.001643440374173224\n",
      "  batch 400 loss: 0.0017729873163625598\n",
      "  batch 450 loss: 0.0018079496501013637\n",
      "  batch 500 loss: 0.0017063301964662968\n",
      "  batch 550 loss: 0.001672734918538481\n",
      "  batch 600 loss: 0.0016710988339036703\n",
      "  batch 650 loss: 0.00161776268389076\n",
      "  batch 700 loss: 0.001618553358130157\n",
      "  batch 750 loss: 0.0016065445845015347\n",
      "  batch 800 loss: 0.0016455364832654595\n",
      "  batch 850 loss: 0.0016477365908212959\n",
      "  batch 900 loss: 0.0016219033859670162\n",
      "  batch 950 loss: 0.001584813732188195\n",
      "  batch 1000 loss: 0.0016338746552355587\n",
      "  batch 1050 loss: 0.0016169032035395503\n",
      "  batch 1100 loss: 0.0016489157336764038\n",
      "LOSS train 0.0016489157336764038 valid 0.0017794828163459897\n",
      "EPOCH 19:\n",
      "  batch 50 loss: 0.0015068594552576542\n",
      "  batch 100 loss: 0.001548475455492735\n",
      "  batch 150 loss: 0.0015489245718345046\n",
      "  batch 200 loss: 0.001601517959497869\n",
      "  batch 250 loss: 0.001575747684109956\n",
      "  batch 300 loss: 0.0016011588159017265\n",
      "  batch 350 loss: 0.0015844039339572191\n",
      "  batch 400 loss: 0.0016645724955014884\n",
      "  batch 450 loss: 0.0015927004045806826\n",
      "  batch 500 loss: 0.001562280748039484\n",
      "  batch 550 loss: 0.0016132111847400666\n",
      "  batch 600 loss: 0.0016167590697295964\n",
      "  batch 650 loss: 0.0016169924521818757\n",
      "  batch 700 loss: 0.0015282835485413671\n",
      "  batch 750 loss: 0.0016265608998946846\n",
      "  batch 800 loss: 0.0016054050344973803\n",
      "  batch 850 loss: 0.001539163098204881\n",
      "  batch 900 loss: 0.0016005949582904577\n",
      "  batch 950 loss: 0.0015845659212209283\n",
      "  batch 1000 loss: 0.0015308577148243786\n",
      "  batch 1050 loss: 0.0016261420748196543\n",
      "  batch 1100 loss: 0.0016228013182990253\n",
      "LOSS train 0.0016228013182990253 valid 0.0018012954387813807\n",
      "EPOCH 20:\n",
      "  batch 50 loss: 0.0015452407184056937\n",
      "  batch 100 loss: 0.0015465986402705312\n",
      "  batch 150 loss: 0.0015427287598140538\n",
      "  batch 200 loss: 0.0015035994630306958\n",
      "  batch 250 loss: 0.0015384639450348914\n",
      "  batch 300 loss: 0.0015305992658250034\n",
      "  batch 350 loss: 0.001516809230670333\n",
      "  batch 400 loss: 0.0015504883602261543\n",
      "  batch 450 loss: 0.001509186786133796\n",
      "  batch 500 loss: 0.001530437746550888\n",
      "  batch 550 loss: 0.0015430231392383575\n",
      "  batch 600 loss: 0.0015513669908978046\n",
      "  batch 650 loss: 0.0016180640971288085\n",
      "  batch 700 loss: 0.001577530091162771\n",
      "  batch 750 loss: 0.0015585121279582382\n",
      "  batch 800 loss: 0.0015565407858230173\n",
      "  batch 850 loss: 0.0016075827227905392\n",
      "  batch 900 loss: 0.001599730127491057\n",
      "  batch 950 loss: 0.001544807709287852\n",
      "  batch 1000 loss: 0.0016072543291375042\n",
      "  batch 1050 loss: 0.001552111553028226\n",
      "  batch 1100 loss: 0.0016114868130534888\n",
      "LOSS train 0.0016114868130534888 valid 0.001800572732463479\n",
      "EPOCH 21:\n",
      "  batch 50 loss: 0.0014717007149010898\n",
      "  batch 100 loss: 0.0015368670597672462\n",
      "  batch 150 loss: 0.0015189588209614158\n",
      "  batch 200 loss: 0.0014899273589253426\n",
      "  batch 250 loss: 0.0015519473422318697\n",
      "  batch 300 loss: 0.0015195350418798625\n",
      "  batch 350 loss: 0.0015000646281987428\n",
      "  batch 400 loss: 0.001469023663084954\n",
      "  batch 450 loss: 0.0015216466900892555\n",
      "  batch 500 loss: 0.0015186984720639883\n",
      "  batch 550 loss: 0.001585905458778143\n",
      "  batch 600 loss: 0.0015549820545129479\n",
      "  batch 650 loss: 0.0015679479227401316\n",
      "  batch 700 loss: 0.001532383752055466\n",
      "  batch 750 loss: 0.0015673017199151218\n",
      "  batch 800 loss: 0.001592283493373543\n",
      "  batch 850 loss: 0.0016353106196038425\n",
      "  batch 900 loss: 0.0015754684270359576\n",
      "  batch 950 loss: 0.001554096369072795\n",
      "  batch 1000 loss: 0.0015885573392733932\n",
      "  batch 1050 loss: 0.0015612934716045857\n",
      "  batch 1100 loss: 0.0015631822729483247\n",
      "LOSS train 0.0015631822729483247 valid 0.0018363074632361531\n",
      "EPOCH 22:\n",
      "  batch 50 loss: 0.002218913969118148\n",
      "  batch 100 loss: 2999471.0926240985\n",
      "  batch 150 loss: 0.003846936635673046\n",
      "  batch 200 loss: 0.003839258197695017\n",
      "  batch 250 loss: 0.003832620517350733\n",
      "  batch 300 loss: 0.0038863643491640685\n",
      "  batch 350 loss: 0.003883461859077215\n",
      "  batch 400 loss: 0.003864792985841632\n",
      "  batch 450 loss: 0.003894855179823935\n",
      "  batch 500 loss: 0.0038580076536163686\n",
      "  batch 550 loss: 0.00391632616519928\n",
      "  batch 600 loss: 0.0038354841712862253\n",
      "  batch 650 loss: 0.00394595988560468\n",
      "  batch 700 loss: 0.003917496376670897\n",
      "  batch 750 loss: 0.0038848084351047873\n",
      "  batch 800 loss: 0.003897591149434447\n",
      "  batch 850 loss: 0.0039109612070024015\n",
      "  batch 900 loss: 0.00394382169470191\n",
      "  batch 950 loss: 0.003868261016905308\n",
      "  batch 1000 loss: 0.0039009766047820448\n",
      "  batch 1050 loss: 0.003884913749061525\n",
      "  batch 1100 loss: 0.0038512620190158486\n",
      "LOSS train 0.0038512620190158486 valid 0.003547572996467352\n",
      "EPOCH 23:\n",
      "  batch 50 loss: 0.0038242070376873017\n",
      "  batch 100 loss: 0.003851166688837111\n",
      "  batch 150 loss: 0.003824123740196228\n",
      "  batch 200 loss: 0.0038996816799044608\n",
      "  batch 250 loss: 0.003907651910558343\n",
      "  batch 300 loss: 0.003917061453685164\n",
      "  batch 350 loss: 0.0038589621521532536\n",
      "  batch 400 loss: 0.0038623700570315123\n",
      "  batch 450 loss: 0.0038739056745544075\n",
      "  batch 500 loss: 0.003970907838083804\n",
      "  batch 550 loss: 0.003888611067086458\n",
      "  batch 600 loss: 0.0038951249746605756\n",
      "  batch 650 loss: 0.003879351322539151\n",
      "  batch 700 loss: 0.0039006406906992195\n",
      "  batch 750 loss: 0.0038757117791101336\n",
      "  batch 800 loss: 0.0037909009493887424\n",
      "  batch 850 loss: 0.0039301702613011\n",
      "  batch 900 loss: 0.003933036648668349\n",
      "  batch 950 loss: 0.0038898633792996407\n",
      "  batch 1000 loss: 0.003913665222935379\n",
      "  batch 1050 loss: 0.003921308945864439\n",
      "  batch 1100 loss: 0.0039135040435940025\n",
      "LOSS train 0.0039135040435940025 valid 0.003547552740201354\n",
      "EPOCH 24:\n",
      "  batch 50 loss: 0.003906648135744035\n",
      "  batch 100 loss: 0.0039177478663623335\n",
      "  batch 150 loss: 0.0039277619495987895\n",
      "  batch 200 loss: 0.0038946138275787233\n",
      "  batch 250 loss: 0.0038547792145982385\n",
      "  batch 300 loss: 0.003938150531612336\n",
      "  batch 350 loss: 0.0039203989878296855\n",
      "  batch 400 loss: 0.00388590142596513\n",
      "  batch 450 loss: 0.003853714489378035\n",
      "  batch 500 loss: 0.003928350573405623\n",
      "  batch 550 loss: 0.00377793665509671\n",
      "  batch 600 loss: 0.003892902280203998\n",
      "  batch 650 loss: 0.0038134474167600273\n",
      "  batch 700 loss: 0.00391780611127615\n",
      "  batch 750 loss: 0.0038930987287312748\n",
      "  batch 800 loss: 0.003922702041454613\n",
      "  batch 850 loss: 0.0039012052351608873\n",
      "  batch 900 loss: 0.003874179944396019\n",
      "  batch 950 loss: 0.0038109612697735428\n",
      "  batch 1000 loss: 0.0038964576739817856\n",
      "  batch 1050 loss: 0.0039339117566123604\n",
      "  batch 1100 loss: 0.0038521899236366153\n",
      "LOSS train 0.0038521899236366153 valid 0.0035475431941449642\n",
      "EPOCH 25:\n",
      "  batch 50 loss: 0.003899443163536489\n",
      "  batch 100 loss: 0.0038438496459275483\n",
      "  batch 150 loss: 0.00394411294721067\n",
      "  batch 200 loss: 0.003918259521014988\n",
      "  batch 250 loss: 0.003934099916368723\n",
      "  batch 300 loss: 0.0038625036366283895\n",
      "  batch 350 loss: 0.003917773687280714\n",
      "  batch 400 loss: 0.003899354767054319\n",
      "  batch 450 loss: 0.0038749559037387373\n",
      "  batch 500 loss: 0.0038437212631106375\n",
      "  batch 550 loss: 0.0038982584606856107\n",
      "  batch 600 loss: 0.003872132059186697\n",
      "  batch 650 loss: 0.0038844403671100735\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m run_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(FCN8_model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      8\u001b[0m model_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(FCN8_model_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m core\u001b[38;5;241m.\u001b[39mtrain_model(model, optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss_fn\u001b[38;5;241m=\u001b[39mloss_fn, epochs\u001b[38;5;241m=\u001b[39mEPOCHS, run_dir\u001b[38;5;241m=\u001b[39mrun_dir, model_dir\u001b[38;5;241m=\u001b[39mmodel_dir)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\repos\\ECE579\\notebooks\\..\\src\\core.py:239\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, optimizer, loss_fn, epochs, run_dir, model_dir)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;66;03m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[0;32m    238\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 239\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, optimizer, loss_fn, epoch_number, writer)\n\u001b[0;32m    242\u001b[0m running_vloss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Set the model to evaluation mode, disabling dropout and using population\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# statistics for batch normalization.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\repos\\ECE579\\notebooks\\..\\src\\core.py:202\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, loss_fn, epoch_index, tb_writer)\u001b[0m\n\u001b[0;32m    199\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# Make predictions for this batch\u001b[39;00m\n\u001b[1;32m--> 202\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    204\u001b[0m \u001b[38;5;66;03m# Compute the loss and its gradients\u001b[39;00m\n\u001b[0;32m    205\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, labels)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\repos\\ECE579\\notebooks\\..\\src\\UNETMS.py:208\u001b[0m, in \u001b[0;36mUNETMS.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    206\u001b[0m t9 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layer_9_transposed(x8)\n\u001b[0;32m    207\u001b[0m c9 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcat_9(x1, t9)\n\u001b[1;32m--> 208\u001b[0m x9 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_layer_9(c9)\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# print('x9 shape ', x9.shape)\u001b[39;00m\n\u001b[0;32m    210\u001b[0m x10 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x9)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    457\u001b[0m                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\utils\\_device.py:77\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[1;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     76\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\site-packages\\torch\\fx\\traceback.py:68\u001b[0m, in \u001b[0;36mformat_stack\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [current_meta\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstack_trace\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# fallback to traceback.format_stack()\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m traceback\u001b[38;5;241m.\u001b[39mformat_list(traceback\u001b[38;5;241m.\u001b[39mextract_stack()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\traceback.py:232\u001b[0m, in \u001b[0;36mextract_stack\u001b[1;34m(f, limit)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m f \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     f \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39m_getframe()\u001b[38;5;241m.\u001b[39mf_back\n\u001b[1;32m--> 232\u001b[0m stack \u001b[38;5;241m=\u001b[39m StackSummary\u001b[38;5;241m.\u001b[39mextract(walk_stack(f), limit\u001b[38;5;241m=\u001b[39mlimit)\n\u001b[0;32m    233\u001b[0m stack\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stack\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\traceback.py:395\u001b[0m, in \u001b[0;36mStackSummary.extract\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f, lineno \u001b[38;5;129;01min\u001b[39;00m frame_gen:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m f, (lineno, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m klass\u001b[38;5;241m.\u001b[39m_extract_from_extended_frame_gen(\n\u001b[0;32m    396\u001b[0m     extended_frame_gen(), limit\u001b[38;5;241m=\u001b[39mlimit, lookup_lines\u001b[38;5;241m=\u001b[39mlookup_lines,\n\u001b[0;32m    397\u001b[0m     capture_locals\u001b[38;5;241m=\u001b[39mcapture_locals)\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\traceback.py:434\u001b[0m, in \u001b[0;36mStackSummary._extract_from_extended_frame_gen\u001b[1;34m(klass, frame_gen, limit, lookup_lines, capture_locals)\u001b[0m\n\u001b[0;32m    430\u001b[0m     result\u001b[38;5;241m.\u001b[39mappend(FrameSummary(\n\u001b[0;32m    431\u001b[0m         filename, lineno, name, lookup_line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mlocals\u001b[39m\u001b[38;5;241m=\u001b[39mf_locals,\n\u001b[0;32m    432\u001b[0m         end_lineno\u001b[38;5;241m=\u001b[39mend_lineno, colno\u001b[38;5;241m=\u001b[39mcolno, end_colno\u001b[38;5;241m=\u001b[39mend_colno))\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m fnames:\n\u001b[1;32m--> 434\u001b[0m     linecache\u001b[38;5;241m.\u001b[39mcheckcache(filename)\n\u001b[0;32m    435\u001b[0m \u001b[38;5;66;03m# If immediate lookup was desired, trigger lookups now.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lookup_lines:\n",
      "File \u001b[1;32mc:\\Users\\Angel Rivera\\.conda\\envs\\windows_env\\Lib\\linecache.py:72\u001b[0m, in \u001b[0;36mcheckcache\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m   \u001b[38;5;66;03m# no-op for files loaded via a __loader__\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 72\u001b[0m     stat \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstat(fullname)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     cache\u001b[38;5;241m.\u001b[39mpop(filename, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "loss_fn = core.get_loss_fn()\n",
    "optimizer = core.get_optimizer(model)\n",
    "EPOCHS = 60\n",
    "cwd = os.getcwd()\n",
    "FCN8_model_dir = os.path.join(cwd, '../models', 'UNETMS_FINAL_NO_AUG')\n",
    "\n",
    "run_dir = os.path.join(FCN8_model_dir, 'run')\n",
    "model_dir = os.path.join(FCN8_model_dir, 'model')\n",
    "\n",
    "core.train_model(model, optimizer=optimizer, loss_fn=loss_fn, epochs=EPOCHS, run_dir=run_dir, model_dir=model_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ece_579_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
